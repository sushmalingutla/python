change-data-capture-app/src/main/python/brute_force_cdc.py
from __future__ import print_functionfrom subprocess import check_outputimport sysfrom pyspark.sql import SparkSessionimport simplejson as json
def execute_cdc_process(spark, table_name, cdc_json_file_path, process_key, hql_directory_path):    """ This method will call the 3 methods to execute the change data capture process and load the        current and history view            1.  generate_cdc_table - Create the data frame with rank calculated for each record based on the primary key and store it as a table.           2. insert_current_partition - Insert the new current partition with the latest changes.           3. insert_history_partition - Moved the Changed records to history partition. """
    #Set Hive dynamic partition parameters    spark.sql("set hive.exec.dynamic.partition=true")    spark.sql("set hive.exec.dynamic.partition.mode=nonstrict")        #parse cdc json file    cdc_table_dictionary = None    cdc_specification = json.load(open(cdc_json_file_path))        cdc_global_dictionary = cdc_specification["changeDataCaptureSpecification"]["global"]    for cdc_table_details_dictionary in cdc_specification["changeDataCaptureSpecification"]["tableDetails"]:                  if table_name == cdc_table_details_dictionary["name"]:              print(table_name + " found in the cdc json file" + cdc_json_file_path)              cdc_table_dictionary = cdc_table_details_dictionary              break                    if cdc_table_dictionary is None:        print(table_name + " not found in the cdc json file " + cdc_json_file_path)        sys.exit(1)               generate_cdc_table(spark, table_name, hql_directory_path)    if "duplicatesCheck" in cdc_table_dictionary and cdc_table_dictionary["duplicatesCheck"].upper() == 'Y':        check_for_duplicates(spark, table_name)            insert_history_partition(spark, table_name, cdc_global_dictionary, cdc_table_dictionary, hql_directory_path)    insert_current_partition(spark, table_name, cdc_global_dictionary, cdc_table_dictionary, process_key, hql_directory_path)        
def generate_cdc_table(spark, table_name, hql_directory_path):    """ This method will get the cdc hql from hdfs, execute the hql in spark and save it as table."""            cdc_hql_file_path = hql_directory_path + "/" + table_name + "_cdc.hql"    #Get the CDC Hql from HDFS    cdc_hql=check_output(["hdfs","dfs","-cat",cdc_hql_file_path])        #Execute the CDC HQL and store the result in a data frame    cdc_df=spark.sql(cdc_hql).persist()        #Save the dataframe as a table.    cdc_df.createOrReplaceTempView(table_name)    
def check_for_duplicates(spark, table_name):    """ This method will check if any 2 records has same rank in the cdc dataframe"""        #Create the dataframe with duplicates count    duplicate_count_hql_file_path = hql_directory_path + "/" + table_name + "_duplicates_check.hql"    #Get the Hql from HDFS    duplicate_count_hql=check_output(["hdfs","dfs","-cat",duplicate_count_hql_file_path])     duplicate_count_df=spark.sql(duplicate_count_hql)    duplicate_count=duplicate_count_df.select("count").rdd.map(lambda row : row[0]).first()    print("Duplicate Count=" + str(duplicate_count))    #Check for duplicate count and fail the process        if duplicate_count > 0:        print("Duplicates are present in the CDC dataframe " + table_name + " and Stopping the process")        sys.exit(10)
    def insert_current_partition(spark, table_name, cdc_global_dictionary, cdc_table_dictionary, process_key, hql_directory_path):    """ This method will  pull the records from temp table and load into the current partition"""        total_current_partition_files = int(cdc_table_dictionary["totalCurrentPartitionFiles"])    partition_column = cdc_table_dictionary["partitionColumn"]    etl_master_table = cdc_table_dictionary["etlMasterTable"]    etl_database = cdc_global_dictionary["etlDatbase"]    master_load_scratch_space = cdc_global_dictionary["masterLoadHdfsScratchSpace"]        if master_load_scratch_space.upper() == 'Y':        master_load_table = "tcdc_hdfs_" + table_name.strip()    else:        master_load_table = etl_master_table        current_partition_hql_path = hql_directory_path + "/" + table_name + "_cdc_current.hql"        #Get the hql from hdfs      current_partition_hql=check_output(["hdfs","dfs","-cat",current_partition_hql_path])            #Run the hql in the spark, coalesce and create the temporary view    spark.sql(current_partition_hql).coalesce(total_current_partition_files).createOrReplaceTempView("current_df")        #Create and run the insert statement    insert_current_partition_hql = "INSERT OVERWRITE TABLE " + etl_database + "." + master_load_table + " PARTITION (" + partition_column + ") select *, 'current_" + process_key + "' as "  + partition_column + " from current_df "           spark.sql(insert_current_partition_hql)        def insert_history_partition(spark, table_name, cdc_global_dictionary, cdc_table_dictionary, hql_directory_path):    """ This method will pull the records  from temp table and load into the history partition"""        total_history_partition_files = int(cdc_table_dictionary["totalHistoryPartitionFiles"])    partition_column = cdc_table_dictionary["partitionColumn"]    etl_master_table = cdc_table_dictionary["etlMasterTable"]    etl_database = cdc_global_dictionary["etlDatbase"]    master_load_scratch_space = cdc_global_dictionary["masterLoadHdfsScratchSpace"]        if master_load_scratch_space.upper() == 'Y':        master_load_table = "tcdc_hdfs_" + table_name.strip()    else:        master_load_table = etl_master_table
    history_partition_hql_path = hql_directory_path + "/" + table_name + "_cdc_history.hql"           #Get the hql from hdfs    history_partition_hql=check_output(["hdfs","dfs","-cat",history_partition_hql_path])        #Run the hql in the spark to create the temporary view.    spark.sql(history_partition_hql).coalesce(total_history_partition_files).createOrReplaceTempView("history_df")        #Create and run the insert statement    insert_history_partition_hql = "INSERT INTO TABLE " + etl_database + "." + master_load_table + " PARTITION (" + partition_column + ") select *, 'history' as " + partition_column  + " from history_df "           spark.sql(insert_history_partition_hql)        
if __name__ == "__main__":    # Initialize    application_name = None    table_name = None    hql_directory_path = None    cdc_json_file_path = None    process_key = None        #Assign the input parameters passed        application_name = sys.argv[1]    table_name = sys.argv[2]    hql_directory_path = sys.argv[3]    cdc_json_file_path = sys.argv[4]    process_key = sys.argv[5]        #Print the input parameters    print("application_name: " + application_name)    print("table_name: " + table_name)    print("hql_directory_path: " + hql_directory_path)    print("cdc_json_file_path: " + cdc_json_file_path)     print("process_key: " + process_key)                               #Create a spark session      spark = SparkSession \            .builder \            .appName(application_name) \            .enableHiveSupport() \            .getOrCreate()
    #Execute the Change Data Capture Process        execute_cdc_process(spark, table_name, cdc_json_file_path, process_key, hql_directory_path)        #Stop the spark session    spark.stop()

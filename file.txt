properties_reader.py
import datetime
def get_properties(path):    properties_dict={}    with open(path,'r') as f:        for line in f:            if (len(str(line)) >1 and line[0] !='#'):                elements=line.rsplit("=")                if(len(elements) > 1):                   key=elements[0]                   value=elements[1].replace("\n","").rstrip()                   properties_dict[key]=value                  f.close()            return  properties_dict    def get_properties_dict(args):    """ This function takes in a list of command line arguments and adds or updates values in the properties file        accordingly.                 Parameters:         args: a list of arguments from command line must be in the format key=value (ex. current_date=20160913)                Return:        properties_dict: the new properties dictionary with updated values     """    new_properties_dict = {}        path = args[0]        properties_dict = get_properties(path)        properties_dict.update( {'workflow_id': 'Default'} )        if 'current_date' in properties_dict:        new_properties_dict['current_date'] = properties_dict.get('current_date')    else:            new_properties_dict['current_date'] = datetime.datetime.strptime(str(datetime.date.today()), "%Y-%m-%d").strftime('%Y%m%d')        for arg in args[1:]:                new_properties_dict.update(dict([arg.split('=')]))
        properties_dict.update(new_properties_dict)        return properties_dict
        
debugutils.py
"""debugutils is used to display the debug information for any dataframe or rdd
This module has the following function(s)--
debug_info : calls debug_query_info, debug_dataframe_info, debug_rdd_info respectively
debug_query_info: Handles printing for the query, count, and dataframe name
debug_rdd_info: Handles printing for the rdd_name, and rdd
debug_dataframe_info: handles printing for the dataframe name and dataframe, but not the query.
is_debug: This function will check if debug_flag is true or false in the properties dictionary.
"""
def debug_info(*args):    """ debug_info : This function calls the respective print function as defined below.         It can take 2 to 3 arguments based on whether or not a dataframe        or rdd is used.             Inputs:        arg[0]: rdd or dataframe name        arg[1]: dataframe_query, rdd, or dataframe        arg[2]: dataframe      """        type_of_arg1 = str(type(args[1]))        if len(args) > 2:                dataframe_name = args[0]        dataframe_query = args[1]        dataframe = args[2]                debug_query_info(dataframe_name, dataframe_query, dataframe)            else:                if type_of_arg1.find("rdd") >= 0:                        rdd_name = args[0]            rdd = args[1]                        debug_rdd_info(rdd_name, rdd)                elif type_of_arg1.find("dataframe") >= 0:                        dataframe_name = args[0]            dataframe = args[1]                        debug_dataframe_info(dataframe_name, dataframe)            def debug_using_temp_table(dataframe_name, dataframe_query, dataframe, hivecontext):    dataframe.registerTempTable("tempDataFrame")    tempDataFrame = hivecontext.sql("SELECT * FROM {0}".format('tempDataFrame'))
    print "****************"    print dataframe_name + " TEMP TABLE QUERY IS :"    print dataframe_query    print "COUNT OF TEMP DF OF",dataframe_name,"=",tempDataFrame.count()    print "SAMPLE RECORDS FROM   " + dataframe_name    print tempDataFrame.show()    print "****************"    hivecontext.sql("DROP TABLE {0}".format('tempDataFrame'))
def debug_query_info(dataframe_name, dataframe_query, dataframe, hivecontext=None):    if hivecontext != None:        debug_using_temp_table(dataframe_name, dataframe_query, dataframe, hivecontext)        return
    """ This function is used to print the debug info for a dataframe with it's query                Inputs:         dataframe_name        dataframe_query        dataframe    """
    print "****************"    print dataframe_name + " QUERY IS :"    print dataframe_query    print "COUNT OF " + dataframe_name + " = " + str(dataframe.count())    print "SAMPLE RECORDS FROM   " + dataframe_name    print dataframe.show()    print "****************"
    def debug_rdd_info(rdd_name, rdd):    """ This function is used to print the debug info for an rdd                Inputs:         rdd_name        rdd    """        print "++++++++++++++++++++++++++++++++++++++++++++++"    print rdd_name + " :"    print "COUNT OF " + rdd_name + " = " + str( rdd.count() )    print "SAMPLE RECORDS : "    print rdd.take(1)    print "++++++++++++++++++++++++++++++++++++++++++++++"    def debug_dataframe_info(dataframe_name, dataframe):    """ This function is used to print the debug info for a dataframe without it's query                Inputs:         dataframe_name        dataframe    """        print "++++++++++++++++++++++++++++++++++++++++++++++"    print dataframe_name + " :"    print "COUNT OF " + dataframe_name + " = " + str( dataframe.count() )    print "SAMPLE RECORDS : "    print dataframe.show()    print "++++++++++++++++++++++++++++++++++++++++++++++"    def is_debug( properties_dict ):    """ This function will check if debug is true or false        Input: properties_dict    """    if( properties_dict is not None and properties_dict.get( 'debug_flag' ) == "TRUE" ):        return True    return False

dateutils.py
"""dateutils is a collection of utilities for use to help manipulate date objects.
calculateWindowDateRange : returns a list that contains the window date range based upon input of months, and the date to begin from
calculateDynamicDateRange : returns a list that contains the window date range based upon input of months, based upon date of execution
"""import timeimport datetimefrom dateutil.relativedelta import relativedelta
def calculate_window_date_range(monthperiod, current_date):    """Name :CalculateWindowDateRange    Description:This function takes Input of month period Variable and return a list which has start date and end date    Input:monthperiod(How many months period)    Output:List[<startdate>,<enddate>)    Example:
    Assume current date is  20160302(YYYYMMDD)    case 1:    Input :if we call CalculateWindowDateRange(2)    Output :it will return [20160101,20160229]
    case 2:if we call CalculateWindowDateRange(12)    Output:it will return [20160229,20150301]
    you can access  startdate and enddate with min and max methods on list    """
    if(monthperiod == 0):        return []    datevalues = []    finalresult = []
    for i in range(0, monthperiod + 1):        current_monthstart = current_date.replace(day = 1)        current_monthstart_string = current_monthstart.strftime("%Y%m%d")        previous_monthlast = (current_monthstart                              - datetime.timedelta(days = 1))        previous_monthlast_string = previous_monthlast.strftime("%Y%m%d")        current_date = previous_monthlast        datevalues.append(current_monthstart_string)        datevalues.append(previous_monthlast_string)
    maxvalue1 = max(datevalues)    minvalue1 = min(datevalues)    datevalues.remove(maxvalue1)    datevalues.remove(minvalue1)    finalmax = max(datevalues)    finalmin = min(datevalues)    finalresult.append(int(finalmin))    finalresult.append(int(finalmax))    return finalresult
def calculate_dynamic_date_range(monthperiod):    """Name :calculateDynamicDateRange    Description:This function takes Input of month period Variable and return a list which has start date and end date    Input:monthperiod(How many months period)    Output:List[<startdate>,<enddate>)    Example:
    Assume current date is  20160309(YYYYMMDD)    case 1:    Input :if we call CalculateDynamicDateRange(1)    Output :it will return [20160209, 20160309]
    case 2:if we call CalculateDynamicDateRange(12)    Output:it will return [20150309, 20160309]
    you can access startdate and enddate with min and max methods on list    """
    if(monthperiod == 0):        return []    finalresult = []    end_date = datetime.date.today()    start_date = end_date + relativedelta(months = -monthperiod)    finalresult.append(int(start_date.strftime("%Y%m%d")))    finalresult.append(int(end_date.strftime("%Y%m%d")))
    return finalresult


sparkutils.py

import datetimeimport osimport os.path
def quiet_logs( sc ):    """ quiet_logs : This function helps to display only print results                 Avoids all INFO and WARN messages"""    logger = sc._jvm.org.apache.log4j    logger.LogManager.getLogger( "org" ). setLevel( logger.Level.ERROR )    logger.LogManager.getLogger( "akka" ).setLevel( logger.Level.ERROR )
def addAllPyFiles(sc, dataItemPath='/home/hadoop/flagship-pas/python/data_items/', utilPath='/home/hadoop/flagship-pas/common/pyspark-lib/src/cmn-lib/'):    addAllDataItemFiles(sc, path=dataItemPath)    addAllUtilityFiles(sc, path=utilPath)
def addAllDataItemFiles(sc, path='/home/hadoop/flagship-pas/python/data_items/'):    """This function is used to add all data-item py files to the spark context"""    for file in os.listdir(path):        sc.addPyFile(path + str(file))
def addAllUtilityFiles(sc, path='/home/hadoop/flagship-pas/common/pyspark-lib/src/cmn-lib/'):    for file in os.listdir(path):        if "sparkutils" not in file:            sc.addPyFile(path + str(file))     def to_csv_line( data ):    """This function is used to concatenate a collection of strings to a single comma-delimited string.
    Parameters:    Parameter 1 = a record to be converted to a comma-delimited string        Returns:    A single string with comma-delimited values.    """    return ','.join( str( d ) for d in data ).encode( "ascii" )
def to_pipe_line( data ):    """This function is used to concatenate a collection of strings to a single comma-delimited string.
    Parameters:    Parameter 1 = a record to be converted to a comma-delimited string        Returns:    A single string with comma-delimited values.    """    return '|'.join( str( d ) for d in data ).encode( "ascii" )
def save_as_csv(data, path):    """This function is used to save a dataframe or RDD as a CSV file on HDFS.
    Parameters:    Parameter 1 = dataframe or RDD    Parameter 2 = HDFS path to save dataframe    """    data.map(lambda x: to_csv_line(x)) \        .saveAsTextFile(path)
def are_not_None( *parameters ):    """    not_None: This function checks to see if any of the parameters are None. If so, false is returned        Parameters:     Parameters--any number of parameters needed to be checked for the existence of None        Return:    notNone - A boolean result. True if no None values exist, false otherwise    """
    not_None = True
    listOfParameters = list( parameters )
    if( len( parameters ) > 0 ):        for parameter in listOfParameters:            if ( parameter is None ):                not_None = False                break    else:        not_None = False
    return not_None
    
rddutils.py
"""rddutils is a collection of utilities for use to help manipulate RDD objects.
This module has the following function(s)--
make_record_key_value_pair : creates a Key Value Pair from the input parameters passedget_rdd_from_parquet : returns a RDD derived from a parquet filefilter_columns : filters out unused columns in RDDdict_to_pair_rdd : returns the value of the money market fund for a given record"""
import timeimport datetimefrom dateutil.relativedelta import relativedelta
from pyspark.sql import SQLContext    # RDD Utilities module(rddutils)
def get_rdd_from_parquet(sc, filePath):    """This functions returns a RDD that is derived from a Parquet file. 
    Parameters:    Parameter 1 = Spark Context     Parameter 2 = File Path of the Parquet file
    Returns:     A RDD derived from Parquet file.    """    sqlContext = SQLContext(sc)    dataframe = sqlContext.parquetFile(filePath)    derivedRdd = dataframe.rdd \          .map(lambda x: x.asDict())    return derivedRdd
def filter_columns(*args):    """This function can be used to filter out unused columns in your RDD.       Must take in either two or three parameters (Any extra parameters are ignored).       Two parameters will create a filtered RDD
       Three parameters will create a filtered pair RDD       args[0] = Dictionary for a record in a RDD       args[1] = List of keys for the Values portion of your key pair RDD       args[2] = List of keys for the keys portion 
       Returns:       A transformed record    """
    # Initialize variables using arguments    recDict = args[0]    valuesForKeyPairValues = []    keysForKeyPairValues = args[1]    valuesForKeyPairKeys = []    keysForKeyPairKeys = []
    # Check if creating a pair RDD    if len(args) > 2:        keysForKeyPairKeys = args[2]
    # Get all of the required values from the record dictionary     for i in keysForKeyPairValues:        valuesForKeyPairValues.append(recDict.get(i))
    # Check for how many values there are and make either a tuple of values or single value    if (len(valuesForKeyPairValues) == 1):        valuesForKeyPairValues = valuesForKeyPairValues[0]    else:        valuesForKeyPairValues = tuple(valuesForKeyPairValues)
    # If making a pair RDD get the values for the key and put in pair RDD form    if (len(keysForKeyPairKeys) > 0):        for i in keysForKeyPairKeys:            valuesForKeyPairKeys.append(recDict.get(i))
        if (len(valuesForKeyPairKeys) == 1):            valuesForKeyPairKeys = valuesForKeyPairKeys[0]        else:            valuesForKeyPairKeys = tuple(valuesForKeyPairKeys)
        return (valuesForKeyPairKeys, valuesForKeyPairValues)    # Otherwise returns list    else:        return valuesForKeyPairValues
def dict_to_pair_rdd(dict, keys, values):    """This function can be used to filter out unused columns in your RDD and create a pair rdd of tuples.       Parameters:       dict = Dictionary for a record in a RDD       keys = List of keys for the Values portion of your key pair RDD       values = List of keys for the keys portion        Returns a transformed record    """    # Create tuples for key and value    if len(keys) > 0:        rddKeys = tuple(dict[key] for key in keys)        # If there is only one get rid of the tuple         if len(rddKeys) == 1:            rddKeys = rddKeys[0]
    if len(values) > 0:        rddValues = tuple(dict[key] for key in values)        if len(rddValues) == 1:            rddValues = rddValues[0]    else:        rddValues = ()
    return (rddKeys, rddValues)
    
client_filter_comparison_utility.py    
from subprocess import callfrom pyspark import SparkConf, SparkContext, SQLContext, Row, HiveContextimport sysimport os
APP_NAME="client_filter_comparison_utility pyspark Application"
def run_data_item(module, properties_file):    """This will run the data item specificed in the arguments passed into this utility"""    call( ["spark-submit", module, properties_file])
def get_data_item_df(sc, data_item_location):    """This method collects the results of the data item       and massages the data frame so that the first column will always be the poid       followed by the rest of the columns in index order i.e column_1, column_2.       This ensures that all the data from the data item is carried over correctly"""
    """Create data frame from output text file"""    data_item_txt = sc.textFile(data_item_location)    data_item_rdd = data_item_txt.map(lambda k: k.split(","))    data_item_df = data_item_rdd.toDF()
    """Iterate through all the data item columns       Rename the first column 'poid'       Rename the rest of the columns based on their index i.e. column_1, column_2..."""    oldColumns = data_item_df.schema.names    newColumns = []    for idx, column in enumerate(data_item_df.columns) :        if idx == 0:            newColumns.append("poid")        else:            newColumns.append("column_" + str(idx))
    data_item_df = reduce(lambda data_item_df, idx: data_item_df.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), data_item_df)
    return data_item_df
def get_filter_df(hivectx, properties_dict, filter_all):    """Run the filtering module and return the results"""    if filter_all == 'Y':        return comparison_test_client_filter.calculate_retail_clnt_filters(hivectx, properties_dict)        return comparison_test_client_filter_no_iig.calculate_retail_clnt_filters(hivectx, properties_dict)
def filter_module(data_item_df, filter_df, hivectx):    """This will filter the data item results by using an outer join with       the filter data frame."""    data_item_df.registerTempTable("data_item_table")    filter_df.registerTempTable("filter_table")
    filter_sql = """SELECT b.vgi_clnt_id, a.* FROM data_item_table AS a                    RIGHT OUTER JOIN filter_table AS b                    ON a.poid = b.vgi_clnt_id"""
    final_df = hivectx.sql(filter_sql)    return final_df
def output_results(final_df, properties_dict, workflow_location):    """Write the final results to hdfs"""    workflow = properties_dict.get('data_item_dir') + "/comparison_test/filtered_"    dataitem_workflow = workflow_location.split("/")    dataitem_workflow_id = dataitem_workflow[-1]    workflow = workflow + dataitem_workflow_id    final_df.rdd.map(sparkutils.to_csv_line).saveAsTextFile(workflow)
def calculate_filtered_data_item(sc, properties_dict, workflow_location, default_values, filter_all):    """Main method that will...       1. Collect data from data item and generate a data frame from it       2. Collect the filtered client table as a data frame       3. Use the filtered client table to filter out clients from the data item          by using an outer join       4. Output the final results into a new hdfs file"""    hivectx = HiveContext(sc)    data_item_df = get_data_item_df(sc, workflow_location)    filter_df = get_filter_df(hivectx, properties_dict, filter_all)    final_df = filter_module(data_item_df, filter_df, hivectx)    final_df = final_df.na.fill(default_values)    output_results(final_df, properties_dict, workflow_location)
if __name__ == "__main__":    """Execute Client Filter on Data Item"""    conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    sc.addPyFile("/home/hadoop/rbc/python/data_items/properties_reader.py")
    import properties_reader    args = sys.argv[1:]    properties_file = args[0]    workflow_location = args[1]    default_value = args[2]    filter_all = args[3]        properties_dict = properties_reader.get_properties(properties_file)    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'debugutils.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'comparison_test_client_filter_no_iig.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'comparison_test_client_filter.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'vista_pct.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_asset_vista.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_assets_combined.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_assets_for_TA.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_assets_for_brokerage.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_asset_529.py' )    sc.addPyFile( properties_dict.get( 'python_source_score_dir') + 'complex_annuity.py' )    sc.addPyFile('/home/hadoop/rbc/common/pyspark-lib/src/cmn-lib/dateutils.py')        import sparkutils    import debugutils    import comparison_test_client_filter    import comparison_test_client_filter_no_iig    sparkutils.quiet_logs(sc)
    calculate_filtered_data_item(sc, properties_dict, workflow_location, default_value, filter_all)    sc.stop()

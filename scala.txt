sqoop
configuration/import_specification_eng.json
{  "import specification": {    "defaults": {      "sqoop": {        "--connect":"jdbc:db2://db2cdq0g-aws.xxx.com:5032/VGIDQ0G:sslConnection=true;",        "--username":"AHDHQ",        "--password-file":"/emr/app/configuration/pw.ssi",        "flags":[ "--as-avrodatafile", "--relaxed-isolation" ]     },     "oozie_properties":{       "oozie.wf.application.path":"/emr/app/common/data-import-export/oozie/tableImport_workflow.xml", "oozie.use.system.libpath":"true", "application_name":"vgidq0g",        "schema_name":"AVGI07", "data_import_export_path":"/emr/app/common/data-import-export", "cleanse_and_validate":"/emr/app/common/data-import-export/cleanse-and-validate", "id_vault_config":"/emr/app/configuration/idVault.config", "region":"eng", "hive_etl_directory_path":"${nameNode}/emr/data/etl", "options_file_path":"/emr/tmp/options/", "hive_master_location":"s3://vgi-retail-${region}-us-east-1-dna-enterprise/",        "archive_location":"s3://vgi-retail-${region}-us-east-1-dna-raw-enterprise/", "location_of_createTableMetadata_workflow":"${data_import_export_path}/oozie/createTableMetadata_workflow.xml", "location_of_sqoop_import_workflow":"${data_import_export_path}/oozie/sqoopImport_workflow.xml", "location_of_historic_load_workflow":"${data_import_export_path}/oozie/historicLoad_workflow.xml", "location_of_hive_load_workflow":"${data_import_export_path}/oozie/hive_load.xml", "etl_file_format":"avro", "cv_schema_output_location":"/emr/app/schemas/cv", "master_file_format":"parquet",        "cv_database_connection_string":"jdbc:db2://db2cdq0g-aws.vanguard.com:5032/VGIDQ0G:sslConnection=true;", "database_connection_string":"\"jdbc:db2://db2cdq0g-aws.vanguard.com:5032/VGIDQ0G:sslConnection=true;\"", "output_edgenode_avro_dir":"/emr/app/schemas/avsc", "output_hdfs_avro_dir":"${nameNode}/emr/app/schemas/avsc", "propertiesFilePath":"/emr/app/configuration", "historicPropertiesFile":"${propertiesFilePath}/import_historicLoad_workflow_${region}.properties", "import_specification_json":"${propertiesFilePath}/import_specification_${region}.json", "hql_path":"/emr/app/common/data-import-export/hql/", "src_python_directory_path":"/emr/app/src/main/python", "spark_job_name":"Cleanse And Validate_${hive_table}", "spark_opts":"--num-executors 20 --executor-memory 3g --executor-cores 2 --conf spark.dynamicAllocation.enabled=true --conf spark.port.maxRetries=128",        "location_of_cleanse_and_validate_workflow":"/emr/app/common/data-import-export/cleanse-and-validate/oozie/cleanse_and_validate_workflow.xml", "appPrefix":"ent", "hdfs_sqoop_directory_path":"/emr/data/sqoop", "hdfs_schema_file_path":"/emr/app/schemas/cv/${hive_table}", "hdfs_avsc_file_path":"/emr/app/schemas/avsc/${hive_table}.avsc", "hdfs_cleansed_directory_path":"/emr/data/cleansed", "hdfs_rejects_directory_path":"/emr/data/rejects", "file_delimiter":"\",\"", "convert_delimiter_to_unicode_flag":"N", "persistence_flag":"N", "remove_headers_flag":"N", "c_v_input_file_format":"avro", "c_v_output_file_format":"avro" , "hive_query_location":"${data_import_export_path}/script/", "thresholdRecordCount":"5", "hive_execution_engine":"tez"     },      "hive_load": {        "source database":"entetl",        "target database":"entmaster"      }    },    "imports": [       {        "name":"VCIN0260",        "oozie_properties":{},        "sqoop": { "--table":"AVGI07.VCIN0260", "--target-dir":"/emr/data/sqoop/tcin026", "--split-by":"CLNT_ADDR_ID", "-m":"20"},               "hive_load": { "source table":"tcin026", "target table":"tcin026", "type":"overwrite"}      },
       {        "name":"VBRKG_TRAN",        "oozie_properties":{},        "sqoop": { "--table":"AVGI07.VBRKG_TRAN", "--target-dir":"/emr/data/sqoop/tbrkg_tran", "--split-by":"ACCT_ID", "-m":"20", "--incremental":"append", "--check-column":"rcrd_dt"},                "hive_load": { "source table":"tbrkg_tran", "target table":"tbrkg_tran", "type":"insert"}      },
      { "name":"VSBS_PRTCPNT_ALOCN",        "oozie_properties":{},        "sqoop": { "--table":"AVGI07.VSBS_PRTCPNT_ALOCN", "--target-dir":"/emr/data/sqoop/tsbs_prtcpnt_alocn", "--split-by":"SAG_ID", "-m":"20"},        "hive_load": { "source table":"tsbs_prtcpnt_alocn", "target table":"tsbs_prtcpnt_alocn", "type":"overwrite"}        },      {        "name":"VFUNCT",        "oozie_properties":{},        "sqoop": { "--table":"AVGI07.VFUNCT", "--target-dir":"/emr/data/sqoop/tfunct", "--split-by":"funct_id", "-m":"1","--incremental":"append","--check-column":"lst_updtd_ts"},               "hive_load": { "source table":"tfunct", "target table":"tfunct", "type":"insert", "partitions":[{ "name":"data_date", "data type":"STRING", "value":"concat(cast(year(LST_UPDTD_TS) as string),lpad(cast(month(LST_UPDTD_TS) as string),2,\"0\"))"}]}      },       


src/main/scala/com/vanguard/hdm/transaction/tdata_items/DivisionCode_VBA.scala
package com.vanguard.hdm.transaction.tdata_items
import com.vanguard.hdm.cmn.utilities.PropertiesReaderimport org.apache.spark.SparkContextimport org.apache.spark.SparkConfimport org.apache.spark.sql.{ Row, DataFrame }import org.apache.spark.sql.functions.{ udf, trim }import java.util.Propertiesimport org.apache.spark.sql.DataFrameimport com.vanguard.hdm.cmn.utilities.DateUtilityimport org.apache.spark.sql.SQLContextimport org.apache.spark.storage.StorageLevelimport org.apache.spark.sql.SparkSession
/* **************************************************************************** * Module Description: The VBADivision module will calculate the following parameters: acct_id, acct_posn_id, PosnServSegmntCD, and MntryDivsnCD *  * This module has the following function(s) -  * main: Sets app name, sparkContext, and properties_dict * calculate_VBADivision: Sets hiveContext, and executes SQL to calculate the PosnServSgmntCD and MntryDivsnCD per acct_id * writeFiles: Writes out the final DataFrame results as a textFile * getSQL_tacct_brkg: Returns SQL to grab the acct_id and rep_cd from tacct_brkg table * getAppName: Gets the Application Name **************************************************************************** */
object DivisionCode_VBA {
  private var DEBUG: Boolean = false
  def main(args: Array[String]) {    if (args.length < 1) {      System.err.println("Missing properties file")      System.exit(1)    }
    val APP_NAME = getAppName()
    val spark = SparkSession      .builder()      .enableHiveSupport()      .appName(APP_NAME)      .config("spark.sql.parquet.writeLegacyFormat", "true")      .getOrCreate()
    val properties_dict = PropertiesReader.get_properties_dict(args(0), args)
    calculate_vba_division_and_saveAsFile(spark, properties_dict)
    spark.stop()  }
  def calculate_vba_division_and_saveAsFile(spark: SparkSession, properties_dict: Properties) = {
    DEBUG = properties_dict.getProperty("debug_flag").toString().toBoolean
    val final_df = calculate_vba_division_code(spark, properties_dict)    final_df.persist(StorageLevel.DISK_ONLY)    writeFiles(spark, final_df, properties_dict)    final_df.unpersist()  }
  def calculate_vba_division_code(spark: SparkSession, properties_dict: Properties): DataFrame = {    val previousTwoYearsDate = DateUtility.getPastDate(2)    val division_repcode_df = CommonDfsUtility.calculate_divisionCode_on_repCode(spark, properties_dict)    division_repcode_df.persist(StorageLevel.DISK_ONLY)    division_repcode_df.registerTempTable("division_table_vba")
    val vba_division_poid_query = "select distinct v.po_id, s.acct_id, s.acct_posn_id, s.posn_serv_sgmnt_cd , s.mntry_divsn_cd, s.efftv_bgn_dt, s.efftv_end_dt from " +      properties_dict.getProperty("tsag_acct_rlshp_tableName") + " t," + properties_dict.getProperty("vsag_viewName") +      " u," + properties_dict.getProperty("vsag_bus_rlshp_viewName") + " v,  division_table_vba s " +      "where trim(t.rlshp_typ_cd) = 'ACCT' and u.sag_id = t.sag_id " +      "and u.serv_id = 90 and from_unixtime(unix_timestamp(u.efftv_end_dt, 'yyyy-MM-dd'), 'yyyy-MM-dd') > '" + previousTwoYearsDate + "' and v.sag_id = u.sag_id and trim(v.po_role_cd) = 'PRRT' " +      "and from_unixtime(unix_timestamp(v.efftv_end_dt, 'yyyy-MM-dd'), 'yyyy-MM-dd') > '" + previousTwoYearsDate + "' and s.acct_id = t.rlshp_id"
    println(vba_division_poid_query)
    val vba_division_poid_df = spark.sql(vba_division_poid_query)    division_repcode_df.unpersist()
    if (DEBUG) {      vba_division_poid_df.show()    }
    return vba_division_poid_df  }
  def writeFiles(spark: SparkSession, finalDF: DataFrame, properties_dict: Properties) = {    val sqlContext = spark.sqlContext    import sqlContext.implicits._    finalDF.map(_.mkString(",")).rdd.saveAsTextFile(properties_dict.getProperty("data_item_dir") + "/" + properties_dict.getProperty("workflow_id") + "/" + "division_vba")  }
  def getAppName(): String = {    return "Division Code - VBA scala application"  }}

<workflow-app name="SOW_score_workflow"  xmlns="uri:oozie:workflow:0.4"> <start to="score_calculation" />     <action name="score_calculation">  <ssh xmlns="uri:oozie:ssh-action:0.1">   <host>localhost</host>   <command>${score_script_dir}</command>   <args>${python_source_final_score_dir}/score_shareofwallet.py</args>   <args>${job_properties_path}</args>   <args>workflow_id=${wf:id()}</args>  </ssh>  <ok to="fs_makeScoreDir" />  <error to="kill" /> </action>  <action name="fs_makeScoreDir">  <fs>   <mkdir path="${wf:conf('score_csv_dir')}"/>  </fs>  <ok to="ssh_prepareScoreForLoad" />  <error to="kill" /> </action>  <action name="ssh_prepareScoreForLoad">  <ssh xmlns="uri:oozie:ssh-action:0.2">   <host>localhost</host>   <command>${prepare_score_script_dir}</command>   <arg>${data_item_dir}/${wf:id()}/score_sow/</arg>   <arg>${score_header}</arg>   <arg>${score_type_version}</arg>   <arg>${local_csv_dir}</arg>   <arg>${score_csv_dir}</arg>  </ssh>  <ok to="ssh_check_if_score_exists" />  <error to="kill" /> </action>
 <action name="ssh_check_if_score_exists">    <ssh xmlns="uri:oozie:ssh-action:0.1">   <!--  checks wether score csv exists or not -->   <host>localhost</host>   <command>${deploy_script_dir}/check_for_new_score.ksh</command>   <args>${score_csv_dir}</args>   <capture-output />  </ssh>  <ok to="ssh_check_records_of_csv_file" />  <error to="kill" /> </action>
 <action name="ssh_check_records_of_csv_file">    <ssh xmlns="uri:oozie:ssh-action:0.1">   <!--  checks wether score csv exists or not -->   <host>localhost</host>   <command>${deploy_script_dir}/check_for_records.ksh</command>   <args>${score_csv_dir}</args>   <capture-output />  </ssh>  <ok to="decision_continueIfScoreExists" />  <error to="kill" /> </action>  <decision name="decision_continueIfScoreExists">  <switch>   <case to="kill">${wf:actionData('ssh_check_records_of_csv_file')['score_path_records'] eq '0'}</case>   <default to="fs_cleanup_files" />   </switch> </decision>
    <action name="fs_cleanup_files">  <fs>   <delete path="${data_item_dir}/${wf:id()}" />  </fs>  <ok to="ssh_copy_score_output_s3" />  <error to="kill" /> </action> <action name="ssh_copy_score_output_s3">    <ssh xmlns="uri:oozie:ssh-action:0.1">   <!--  exports score csv file to s3  -->   <host>localhost</host>   <command>${deploy_script_dir}/output_results_to_s3.sh</command>   <args>${score_csv_dir}</args>   <args>${aws_s3_score_location}</args>   <capture-output />  </ssh>  <ok to="end" />  <error to="kill" /> </action>  
 <kill name="kill">  <message>Action failed, error   message[${wf:errorMessage(wf:lastErrorNode())}]</message> </kill>
 <end name="end" /></workflow-app>

output_results_to_s3.sh
#!/bin/bash
hdfs_results=$1s3_location=$2
current_date=$(date +%Y%m%d)
echo "date = ${current_date}"echo "***hdfs results path = ${hdfs_results}"echo "***s3 location       = ${s3_location}"echo "date= $current_date "echo "s3-dist-cp --src=${hdfs_results}    --dest=${s3_location}/${current_date}   --srcPattern='.*.*.csv*'    --s3ServerSideEncryption"
s3-dist-cp --src=${hdfs_results}    --dest=${s3_location}/${current_date}   --srcPattern='.*.*.csv*'    --s3ServerSideEncryption

run_score.ksh(scripts)
print "spark-submit -v --master yarn --deploy-mode client --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.yarn.driver.memoryOverhead=13312 --driver-memory 75G --driver-cores 5 --conf spark.dynamicAllocation.enabled=true --conf spark.executor.extraJavaOptions=-XX:+UseG1GC ${1} ${2} ${3} ;"spark-submit -v --master yarn --deploy-mode client --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.yarn.driver.memoryOverhead=13312 --driver-memory 75G --driver-cores 5 --conf spark.dynamicAllocation.enabled=true --conf spark.executor.extraJavaOptions=-XX:+UseG1GC ${1} ${2} ${3} ;

prepare_score_for_load.ksh
#!/bin/ksh
#################### Local functions ####################function usage{  echo "Usage: prepare_score_for_load.ksh <input score path> <header csv path> <score type version> <output score path>"  echo "where"  echo "<input score path> HDFS path to input score file(s)"  echo "<header csv path> HDFS path CSV file containing column name headers"  echo "<score type version> score name in format '<score_type>_<version>'"  echo "<output score path> HDFS path to save score csv"}
######################## Determine arguments ########################INPUT_SCORE_PATH=$1HEADER_CSV_PATH=$2SCORE_TYPE_VERSION="$3"OUTPUT_SCORE_PATH=$4HDFS_CSV_FILE=$5TMP_DIRECTORY=$1/tmp
if [[ "$SCORE_TYPE_VERSION" = "SOW-3.00" ]]; then  SCORE_TYPE_VERSION="SOW-3.00"fi
set +x
################################################################## Add header, combine to single CSV, and store to tmp directory ##################################################################
hadoop fs -getmerge $INPUT_SCORE_PATH/*  /mnt/temp.csv
mkdir $OUTPUT_SCORE_PATH
echo "poid,rig_sow_overall_v3,rig_tot_vngrd_assets_v3,rig_remain_wallet_overall_v3,rig_remain_opp_overall_v3,rig_tot_vngrd_mf_etf_assets_v3,rig_sow_mf_etf_v3,rig_remain_wallet_mf_etf_v3,rig_remain_opp_mf_etf_v3" > header
cat  header  /mnt/temp.csv > $OUTPUT_SCORE_PATH/$SCORE_TYPE_VERSION.csv
hadoop fs -put $OUTPUT_SCORE_PATH/$SCORE_TYPE_VERSION.csv  $HDFS_CSV_FILE
############################ Clean up temp directory ############################rm -r  /mnt/temp.csvexit 0


run_sow_workflow.sh
 #!/bin/bash
############################################ get NameNode Address & IP & AWS Account ############################################source /emr/deploy.outAWS_ACCOUNT=$(echo ${APPLICATION_PAYLOAD_BUCKET} | cut -f5 -d'-')S3_BUCKET_PREFIX="vgi-retail-${AWS_ACCOUNT}-us-east-1"sed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job.properties#sed -i "s~\(aws_s3_score_location=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job.properties
sed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_test.properties#sed -i "s~\(aws_s3_score_location=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_test.properties
sed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_prod.properties#sed -i "s~\(aws_s3_score_location=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_prod.properties
sed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_ENG_job.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_TEST_job.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_PRD_job.properties
ipAddressShort=`hostname`ipAddressFull=`hostname -f`nameNodeAddress=hdfs://${ipAddressFull}:8020jobTrackerAddress=${ipAddressFull}:8032
sed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_test.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_prod.propertiessed -i "s~\(nameNode=\).*~\1${nameNodeAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_ENG_job.propertiessed -i "s~\(jobTracker=\).*~\1${jobTrackerAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_ENG_job.propertiessed -i "s~\(nameNode=\).*~\1${nameNodeAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_TEST_job.propertiessed -i "s~\(jobTracker=\).*~\1${jobTrackerAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_TEST_job.propertiessed -i "s~\(nameNode=\).*~\1${nameNodeAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_PRD_job.propertiessed -i "s~\(jobTracker=\).*~\1${jobTrackerAddress}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_PRD_job.properties
oozie_sharedlib=$(oozie admin -sharelibupdate | grep sharelibDirNew | awk -F= '{print $NF}' | cut -c 2-)oozie admin -sharelibupdatesed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_test.propertiessed -i "s~\(aws_s3_bucket_prefix=\).*~\1${S3_BUCKET_PREFIX}~g" /home/hadoop/sow/oozie/sow/SOW_job_prod.propertiessed -i "s~\(oozie.libpath=\).*~\1${oozie_sharedlib}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_ENG_job.propertiessed -i "s~\(oozie.libpath=\).*~\1${oozie_sharedlib}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_TEST_job.propertiessed -i "s~\(oozie.libpath=\).*~\1${oozie_sharedlib}~g" /home/hadoop/sow/configuration/HDM_SOW_SCORE_PRD_job.properties
############################################################### create HDFS directories & put application from EMR to HDFS ###############################################################hdfs dfs -mkdir -p /user/hadoop/sow/hdfs dfs -put /home/hadoop/sow/* /user/hadoop/sow/hdfs dfs -chmod -R 777 /user/hadoop/sow/
############################################# function to print SOW Workflow Status #############################################function printOozieInfo {    workflowId=$1    oozieJobInfo=`oozie job -info $workflowId`    echo "$oozieJobInfo"        RunningStep=`echo "$oozieJobInfo" | grep "RUNNING\|ERROR\|KILLED\|FAILED" | tail -n1`    subWorkflow=`echo "$RunningStep" | awk '{print $(3)}'`    if [[ ${subWorkflow: -1} == "-" ]] ; then      subWorkflow=`echo "$subWorkflow" | sed 's/.$//'`    else      subWorkflow=`echo "$subWorkflow" | sed 's/RUNNING//' | sed 's/KILLED//' | sed 's/FAILED//'`    fi    if [[ ${subWorkflow} == *-oozie-oozi-W* ]] ; then       subOozieJobInfo=`oozie job -info $subWorkflow`       echo "$subOozieJobInfo"       SubWorkflowRunningStep=`echo "$subOozieJobInfo" | grep "RUNNING\|ERROR\|KILLED\|FAILED" | tail -n1`       subsubWorkflow=`echo "$SubWorkflowRunningStep" | awk '{print $(3)}'`    if [[ ${subsubWorkflow: -1} == "-" ]] ; then       subsubWorkflow=`echo "$subsubWorkflow" | sed 's/.$//'`    else       subsubWorkflow=`echo "$subsubWorkflow" | sed 's/RUNNING//' | sed 's/KILLED//' | sed 's/FAILED//'`    fi       if [[ ${subsubWorkflow} == *-oozie-oozi-W* ]] ; then           subsubOozieJobInfo=`oozie job -info "$subsubWorkflow"`           echo "$subsubOozieJobInfo"           SubSubWorkflowRunningStep=`echo "$subsubOozieJobInfo" | grep "RUNNING\|ERROR\|KILLED\|FAILED" | tail -n1`           if [[ ${SubSubWorkflowRunningStep} != *RUNNING* ]] ; then               StepID=`echo "$SubSubWorkflowRunningStep" | awk 'FNR==1{print $(1)}'`               StepInfo=`oozie job -info $StepID -verbose`               echo "$StepInfo"           fi       elif [[ ${SubWorkflowRunningStep} != *RUNNING* ]] ; then           StepID=`echo "$SubWorkflowRunningStep" | awk 'FNR==1{print $(1)}'`           StepInfo=`oozie job -info $StepID -verbose`           echo "$StepInfo"       fi    elif [[ ${RunningStep} != *RUNNING* ]] ; then        StepID=`echo "$RunningStep" | awk 'FNR==1{print $(1)}'`        StepInfo=`oozie job -info $StepID -verbose`        echo "$StepInfo"    fi}
function copyOozieLogs() {hdfs dfs -put -f ~/oozie-oozi /user#hadoop fs -cp -f /user/oozie-oozi s3://${S3_BUCKET_PREFIX}-emr/clouderaconversion-models-sow-inf/logs/#LOGDATE=$(date '+%Y%m%d%H%M')LOGDATE=$(date '+%Y%m%d')
if aws s3 sync  --content-type text/plain  ~/oozie-oozi/  s3://${S3_BUCKET_PREFIX}-emr/clouderaconversion-models-sow-inf/logs/$LOGDATE --exclude "*" --include "*.stdout" --include "*.stderr" --sse --quiet ; then    echo "Coping the logs ..." else    echo "Faied to copy the logs"fi}
########################################### run Workflow & Display Progress ############################################ Variable declarationspropertiesFile=/home/hadoop/sow/oozie/sow/SOW_job_${AWS_ACCOUNT}.properties#propertiesFile=/home/hadoop/sow/oozie/sow/SOW_job_test.properties#propertiesFile=/home/hadoop/sow/oozie/sow/SOW_job_prod.properties
workflowId=workflowStatus=
# Execute the Oozie workflowworkflowId=`oozie job -config ${propertiesFile} -run | grep "^job:" | sed "s/^job: //"`echo workflowId=$workflowIdif [ -z "$workflowId" ] ; then echo "SOWMetricsModuleWorkflowStatus=FAILED" echo "No workflow id returned!" echo "End SOW Metrics Module Workflow" exit 1fi
# Loop until the workflow has completedwhile (true) ; do    workflowStatus=`oozie job -info $workflowId | grep "^Status " | awk '{print $(NF)}'`    echo "SOWMetricsModuleWorkflowStatus=$workflowStatus"
    if [[ ${workflowStatus} == "RUNNING" ]] ; then       printOozieInfo $workflowId       copyOozieLogs    elif [[ ${workflowStatus} == "FAILED" ]] || [[ ${workflowStatus} == "KILLED" ]] ; then       printOozieInfo $workflowId       copyOozieLogs       echo "End SOW Metrics Module Workflow"        exit 1    elif [[ ${workflowStatus} == "SUSPENDED" ]] ; then       oozie job -kill $workflowId       printOozieInfo $workflowId       copyOozieLogs       echo "End SOW Metrics Module Workflow"        exit 1    elif [[ ${workflowStatus} == "SUCCEEDED" ]] ; then     printOozieInfo $workflowId     copyOozieLogs     echo "End SOW Metrics Module Workflow"        exit 0    fi
    sleep 30sdone
   
   
   write_logs.sh
#!/bin/ksh
Message=$1WF_ID=$2WF_Name=$3WF_AppPath=$4WF_LastErrorNode=$5WF_ErrorCodeOfLastErrorNode=$6WF_ErrorMessageOfLastErrorNode=$7
File=/home/hadoop/sow/results/results_$WF_ID.txtif [ -f $File ];then    rm $Filefitouch $File    `cat > $File << EOF$MessageWorkflow Details:*************************************************************************************************WorkFlow Id : $WF_IDWorkFlow Name : $WF_NameWorkFlow Path : $WF_AppPathError Node : $WF_LastErrorNodeError Code :$WF_ErrorCodeOfLastErrorNodeError Message :$WF_ErrorMessageOfLastErrorNode**************************************************************************************************`

check_for_records.ksh
#!/bin/ksh#
#################### Local functions ####################function usage{  echo "Usage: check_for_new_score.ksh <score directory path>"  echo "where"  echo "<score directory path> path to directory which contains score csv files"}
######################## Determine arguments ########################SCORE_DIR=$1
########################################################################### SCORE_PATH:                                                            ##    input - score directory path                                        ##   output - hdfs path of score with oldest timestamp in score directory ###########################################################################SCORE_PATH=$(hdfs dfs -ls $1 | tr -s ' ' | cut -d\  -f6,7,8- | sort -k1 -k2 -k3 | sed '/^[[:space:]]*$/d' | head -n1 | cut -d\  -f3-)Records=$(hdfs dfs -cat  /data/sow/etl/staging/sow/sow_score/SOW-3.00.csv | wc -l)
set +x
if [[ -n "Records" ]] ; then  echo "score_path_records=$Records"else  echo "score_path_records=NA"fi
exit 0

run_client_filter_utility.sh
#!/bin/bash
echo "spark-submit --master yarn --deploy-mode client ${1} ${2} ${3} ${4} ${5}"spark-submit --master yarn --conf spark.sql.broadcastTimeout=3600 --conf spark.driver.cores=8 --conf spark.driver.memory=13g --conf spark.executor.memory=30g --conf spark.yarn.executor.memoryOverhead=12g --conf spark.shuffle.memoryFraction=0.6 --conf spark.dynamicAllocation.enabled=false --conf spark.executor.instances=20 --conf spark.network.timeout=800 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --deploy-mode client ${1} ${2} ${3} ${4} ${5};

writeResults.ksh
#!/bin/ksh
Message=$1
if [[ "$Message" = "FAILED: SOW FILE PROCESS Oozie Workflow failed" || "$Message" = "Hive Action Failure" ]];then    WF_ID=$2    WF_Name=$3    WF_AppPath=$4    WF_LastErrorNode=$5    WF_ErrorCodeOfLastErrorNode=$6    WF_ErrorMessageOfLastErrorNode=$7        File=/home/hadoop/sow/results/results_$WF_ID.txt    if [ -f $File ];then        rm $File    fi    touch $File        `cat > $File << EOF$MessageWorkflow Details:*************************************************************************************************WorkFlow Id : $WF_IDWorkFlow Name : $WF_NameWorkFlow Path : $WF_AppPathError Node : $WF_LastErrorNodeError Code :$WF_ErrorCodeOfLastErrorNodeError Message :$WF_ErrorMessageOfLastErrorNode**************************************************************************************************`
 elif [[ "$Message" = "SUCCESS: SOW FILE PROCESS Oozie Workflow completed successfully" ]];then WF_ID=$2    WF_Name=$3    WF_AppPath=$4        File=/home/hadoop/sow/results/results_$WF_ID.txt    if [ -f $File ];then        rm $File    fi    touch $File        `cat > $File << EOF$MessageWorkflow Details:*************************************************************************************************WorkFlow Id : $WF_IDWorkflow Name :  $WF_NameComments : $WF_AppPath**************************************************************************************************`
elif [[ "$Message" = "Rejects threshold exceeded" || "$Message" = "Rejects detected" ]];then Full_Error=$2    WF_ID=$3    rejectsThreshold=$4    rejectsCount=$5    targetRejectsFile=$6        File=/home/hadoop/sow/results/results_$WF_ID.txt    if [ -f $File ];then        rm $File    fi    touch $File        `cat > $File << EOF$MessageWorkflow Details:*************************************************************************************************WorkFlow Id Killed : $WF_IDThreshold :  $rejectsThresholdReject Count : $rejectsCountHDFS Path : $targetRejectsFile**************************************************************************************************`     elif [[ "$Message" = "notify_success" || "$Message" = "notify_failure" || "$Message" = "Hive Action Failure" ]];then Status_Message=$2    WF_ID=$3    comments=$4        File=/home/hadoop/sow/results/results_$WF_ID.txt    if [ -f $File ];then        rm $File    fi    touch $File        `cat > $File << EOF$MessageWorkflow Details:*************************************************************************************************WorkFlow Id : $WF_IDStatus :  $Status_MessageComments : $comments**************************************************************************************************`fi

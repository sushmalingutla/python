src/main/scala/com/vanguard/dax/cmn/cleanse_and_validate/helpers/Schemas.scala
package com.vanguard.dax.cmn.cleanse_and_validate.helpers
import java.io.File
import scala.collection.mutable.ArrayBuffer
import org.apache.spark.SparkContext
class Schemas { private var tables = new ArrayBuffer[String] private var schemaColumns = new ArrayBuffer[Array[String]]  def getTables(): Array[String] = {  tables.toArray }  def getSchemaColumns(): Array[Array[String]] = {  schemaColumns.toArray }  def setTables(tablesData: Array[String]): Unit = {  tables = tables ++= tablesData }  def setSchemaColumns(schemaColumnsData: Array[Array[String]]): Unit = {  schemaColumns = schemaColumns ++= schemaColumnsData }  def mapDataType(avroDataType: String, parquetDataType: String, dataSource: String): String = {  avroDataType.toLowerCase() match {   case "date" => {    dataSource.toLowerCase() match {     case "oracle" => "timestamp"     case _ => "date"    }   }   case "char" | "varchar" => "char"   case "string" => {    parquetDataType.toLowerCase() match {     case "timestamp" => "timestamp"     case _ => "char"    }   }   case "bigint" | "int" | "decimal" | "smallint" => "number"  } }  def getLength(length: String, sourceDataType: String): String = {  val decimalRegex = "\"\\d+,\\d+\"".r  sourceDataType.toLowerCase() match {   case "integer" | "smallint" => calculateSignedIntegerLength(length)   case _ => {    length match {     case decimalRegex(_*) => {      val digits = removeQuotes(length).split(",")      (digits(0).toInt + digits(1).toInt + 1).toString     }     case "" => "500"     case _ => length    }   }  } }  def removeQuotes(field: String): String = {   field.replaceAll("\"","") }  def calculateSignedIntegerLength(length: String): String = {   (Math.pow(2,(length.toInt*8)-1).toInt.toString.length+1).toString }  def saveToLocalFileSystem(path: String, payload: String) : Unit = {  val pw = new java.io.PrintWriter(new File(path))  try pw.write(payload)  finally pw.close() }}
object Schemas { val schemas = new Schemas
 def generate(sc: SparkContext, inputPath: String, columnIndexes: Array[Int]) : Unit = {  val rawData = sc.textFile(inputPath).map(x=>x.split("\t",-1))    val rawTableName = columnIndexes(0)  val rawColumnNumber = columnIndexes(1)  val rawColumnName = columnIndexes(2)  val rawDataType = columnIndexes(3)  val rawLength = columnIndexes(4)  val rawSourceDataType = columnIndexes(5)  val rawParquetDataType = columnIndexes(6)  val rawDataSource = columnIndexes(7)    val columns = rawData.map(column => Array(column(rawTableName),column(rawColumnNumber),column(rawColumnName),   column(rawDataType),column(rawLength),column(rawSourceDataType),   column(rawParquetDataType),column(rawDataSource)))    val tableName = 0  val columnNumber = 1  val columnName = 2  val dataType = 3  val length = 4  val sourceDataType = 5  val parquetDataType = 6  val dataSource = 7    val tables = columns.map(column => column(tableName)).distinct.collect  schemas.setTables(tables)    schemas.setSchemaColumns(columns.map(column=>Array(column(tableName),    column(columnName),    schemas.mapDataType(column(dataType),column(parquetDataType),column(dataSource)),    schemas.getLength(column(length),column(sourceDataType)),column(columnNumber))   )   .collect  )   }  def saveSchemasToLocalFileSysem(sc: SparkContext, directoryPath: String) : Unit = {  val tables = schemas.getTables  val schemaColumns = sc.parallelize(schemas.getSchemaColumns)  val path = directoryPath.endsWith("/") match {   case true => directoryPath   case _ => directoryPath + "/"  }    for (i <- 0 until tables.size) {   val tableSchemaString = schemaColumns.filter(column => column(0) == tables(i))    .map(_.dropRight(1) //drop dataSource     .drop(1) //drop tableName     .mkString("\t")    )    .collect    .mkString("\n")   schemas.saveToLocalFileSystem(path+tables(i).toLowerCase,tableSchemaString)  } }}

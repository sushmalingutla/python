src/main/scala/com/vanguard/dax/cmn/cleanse_and_validate/helpers/Schemas.scala
package com.vanguard.dax.cmn.cleanse_and_validate.helpers
import java.io.File
import scala.collection.mutable.ArrayBuffer
import org.apache.spark.SparkContext
class Schemas { private var tables = new ArrayBuffer[String] private var schemaColumns = new ArrayBuffer[Array[String]]  def getTables(): Array[String] = {  tables.toArray }  def getSchemaColumns(): Array[Array[String]] = {  schemaColumns.toArray }  def setTables(tablesData: Array[String]): Unit = {  tables = tables ++= tablesData }  def setSchemaColumns(schemaColumnsData: Array[Array[String]]): Unit = {  schemaColumns = schemaColumns ++= schemaColumnsData }  def mapDataType(avroDataType: String, parquetDataType: String, dataSource: String): String = {  avroDataType.toLowerCase() match {   case "date" => {    dataSource.toLowerCase() match {     case "oracle" => "timestamp"     case _ => "date"    }   }   case "char" | "varchar" => "char"   case "string" => {    parquetDataType.toLowerCase() match {     case "timestamp" => "timestamp"     case _ => "char"    }   }   case "bigint" | "int" | "decimal" | "smallint" => "number"  } }  def getLength(length: String, sourceDataType: String): String = {  val decimalRegex = "\"\\d+,\\d+\"".r  sourceDataType.toLowerCase() match {   case "integer" | "smallint" => calculateSignedIntegerLength(length)   case _ => {    length match {     case decimalRegex(_*) => {      val digits = removeQuotes(length).split(",")      (digits(0).toInt + digits(1).toInt + 1).toString     }     case "" => "500"     case _ => length    }   }  } }  def removeQuotes(field: String): String = {   field.replaceAll("\"","") }  def calculateSignedIntegerLength(length: String): String = {   (Math.pow(2,(length.toInt*8)-1).toInt.toString.length+1).toString }  def saveToLocalFileSystem(path: String, payload: String) : Unit = {  val pw = new java.io.PrintWriter(new File(path))  try pw.write(payload)  finally pw.close() }}
object Schemas { val schemas = new Schemas
 def generate(sc: SparkContext, inputPath: String, columnIndexes: Array[Int]) : Unit = {  val rawData = sc.textFile(inputPath).map(x=>x.split("\t",-1))    val rawTableName = columnIndexes(0)  val rawColumnNumber = columnIndexes(1)  val rawColumnName = columnIndexes(2)  val rawDataType = columnIndexes(3)  val rawLength = columnIndexes(4)  val rawSourceDataType = columnIndexes(5)  val rawParquetDataType = columnIndexes(6)  val rawDataSource = columnIndexes(7)    val columns = rawData.map(column => Array(column(rawTableName),column(rawColumnNumber),column(rawColumnName),   column(rawDataType),column(rawLength),column(rawSourceDataType),   column(rawParquetDataType),column(rawDataSource)))    val tableName = 0  val columnNumber = 1  val columnName = 2  val dataType = 3  val length = 4  val sourceDataType = 5  val parquetDataType = 6  val dataSource = 7    val tables = columns.map(column => column(tableName)).distinct.collect  schemas.setTables(tables)    schemas.setSchemaColumns(columns.map(column=>Array(column(tableName),    column(columnName),    schemas.mapDataType(column(dataType),column(parquetDataType),column(dataSource)),    schemas.getLength(column(length),column(sourceDataType)),column(columnNumber))   )   .collect  )   }  def saveSchemasToLocalFileSysem(sc: SparkContext, directoryPath: String) : Unit = {  val tables = schemas.getTables  val schemaColumns = sc.parallelize(schemas.getSchemaColumns)  val path = directoryPath.endsWith("/") match {   case true => directoryPath   case _ => directoryPath + "/"  }    for (i <- 0 until tables.size) {   val tableSchemaString = schemaColumns.filter(column => column(0) == tables(i))    .map(_.dropRight(1) //drop dataSource     .drop(1) //drop tableName     .mkString("\t")    )    .collect    .mkString("\n")   schemas.saveToLocalFileSystem(path+tables(i).toLowerCase,tableSchemaString)  } }}
 
 
 
 
 demographics_data_factor.py
 """Calculation of demographic related data items factor by using user defined functionsData factors : 13 items: educ_9514, best_age, inc_fl_higher_rngs_8201, contact_cnt_06, watch_list_fl, lit_req_mm_12, web_reg_fl,                 Nsghts_Nws_Prspctvs_pgvw360, IRAs_sess360, rim_over_yr, rim_been_while, single_fl and female_fl"""from pyspark import SparkConf, SparkContext, SQLContext, Row, HiveContext, StorageLevelfrom pyspark.sql.types import IntegerType,Rowimport hashlibimport sysimport subprocesssys.path.append( '/home/hadoop/rbc/common/pyspark-lib/src/cmn-lib' )sys.path.append( '/home/hadoop/rbc/python/data_factors' )

import cap_contact_cnt_06_factor, cap_educ_9514_factor, cap_inc_fl_higher_rngs_8201_factorimport cap_iras_sess360_factor, cap_lit_req_mm_12_factor, cap_nsghts_nws_prspctvs_pgvw360_factorimport cap_rim_been_while_factor, cap_rim_over_yr_factor, best_age, cap_female_fl, cap_single_flimport cap_watch_list_fl_factor, cap_web_reg_fl_factorimport debugutils, sparkutils, rddutils
APP_NAME="RETAIL DEMOGRAPHICS DATA FACTOR PYSPARK APPLICATION"def run_demographic_datafactors(hive_ctx, properties_dict):    cap_contact_cnt_06_factor.calculate_contact_cnt_06_factor(hive_ctx, properties_dict)    cap_educ_9514_factor.calculate_educ_9514_factor(hive_ctx, properties_dict)    cap_inc_fl_higher_rngs_8201_factor.calculate_inc_fl_higher_rngs_8201_factor(hive_ctx, properties_dict)    cap_iras_sess360_factor.calculate_iras_sess360_factor(hive_ctx, properties_dict)    cap_lit_req_mm_12_factor.calculate_lit_req_mm_12_factor(hive_ctx, properties_dict)    cap_nsghts_nws_prspctvs_pgvw360_factor.calculate_nsghts_nws_prspctvs_pgvw360_factor(hive_ctx, properties_dict)    cap_rim_been_while_factor.calculate_rim_been_while_factor(hive_ctx, properties_dict)    cap_rim_over_yr_factor.calculate_rim_over_yr_factor(hive_ctx, properties_dict)    cap_watch_list_fl_factor.calculate_watch_list_fl_factor(hive_ctx, properties_dict)    cap_web_reg_fl_factor.calculate_web_reg_fl_factor(hive_ctx, properties_dict)    best_age.get_best_age_df(hive_ctx, properties_dict)    cap_female_fl.get_cap_female_fl(hive_ctx, properties_dict)    cap_single_fl.get_cap_single_fl(hive_ctx, properties_dict)    
def calculate_join_1_query(hive_ctx, properties_dict):    cap_educ_9514_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_educ_9514_df_location'))    cap_educ_9514_df.persist(StorageLevel.DISK_ONLY)    cap_educ_9514_df.registerTempTable("cap_educ_9514_table")        cap_inc_fl_higher_rngs_8201_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_inc_fl_higher_rngs_8201_df_location'))    cap_inc_fl_higher_rngs_8201_df.persist(StorageLevel.DISK_ONLY)    cap_inc_fl_higher_rngs_8201_df.registerTempTable("cap_inc_fl_higher_rngs_8201_table")          cap_data_factor_join_1_query = """select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, b.cap_inc_fl_higher_rngs_8201                                      from cap_educ_9514_table a full outer join cap_inc_fl_higher_rngs_8201_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""                                  cap_data_factor_join_1_df = hive_ctx.sql(cap_data_factor_join_1_query)    cap_data_factor_join_1_df.write.save(properties_dict.get('demographics_df_1_location') + '/', format='parquet', mode='overwrite')        cap_educ_9514_df.unpersist()    cap_inc_fl_higher_rngs_8201_df.unpersist()    hive_ctx.sql("DROP TABLE cap_educ_9514_table")    hive_ctx.sql("DROP TABLE cap_inc_fl_higher_rngs_8201_table")
def calculate_join_2_query(hive_ctx, properties_dict):    cap_data_factor_join_1_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_1_location'))    cap_data_factor_join_1_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_1_df.registerTempTable("cap_data_factor_join_1_table")        cap_contact_cnt_06_df  = hive_ctx.read.format('parquet').load(properties_dict.get('cap_contact_cnt_06_df_location'))    cap_contact_cnt_06_df.persist(StorageLevel.DISK_ONLY)    cap_contact_cnt_06_df.registerTempTable("cap_contact_cnt_06_table")        cap_data_factor_join_2_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, b.cap_contact_cnt_06                                      from cap_data_factor_join_1_table a full outer join cap_contact_cnt_06_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_2_df = hive_ctx.sql(cap_data_factor_join_2_query)    cap_data_factor_join_2_df.write.save(properties_dict.get('demographics_df_2_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_1_df.unpersist()    cap_contact_cnt_06_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_1_table")    hive_ctx.sql("DROP TABLE cap_contact_cnt_06_table")    subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_1_location')])
def calculate_join_3_query(hive_ctx, properties_dict):    cap_data_factor_join_2_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_2_location'))    cap_data_factor_join_2_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_2_df.registerTempTable("cap_data_factor_join_2_table")        cap_watch_list_fl_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_watch_list_fl_df_location'))    cap_watch_list_fl_df.persist(StorageLevel.DISK_ONLY)    cap_watch_list_fl_df.registerTempTable("cap_watch_list_fl_table")        cap_data_factor_join_3_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        b.cap_watch_list_fl                                        from cap_data_factor_join_2_table a full outer join cap_watch_list_fl_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_3_df = hive_ctx.sql(cap_data_factor_join_3_query)    cap_data_factor_join_3_df.write.save(properties_dict.get('demographics_df_3_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_2_df.unpersist()    cap_watch_list_fl_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_2_table")    hive_ctx.sql("DROP TABLE cap_watch_list_fl_table")    subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_2_location')])
def calculate_join_4_query(hive_ctx, properties_dict):    cap_data_factor_join_3_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_3_location'))    cap_data_factor_join_3_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_3_df.registerTempTable("cap_data_factor_join_3_table")        cap_lit_req_mm_12_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_lit_req_mm_12_df_location'))    cap_lit_req_mm_12_df.persist(StorageLevel.DISK_ONLY)    cap_lit_req_mm_12_df.registerTempTable("cap_lit_req_mm_12_table")         cap_data_factor_join_4_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, b.cap_lit_req_mm_12                                        from cap_data_factor_join_3_table a full outer join cap_lit_req_mm_12_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_4_df = hive_ctx.sql(cap_data_factor_join_4_query)    cap_data_factor_join_4_df.write.save(properties_dict.get('demographics_df_4_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_3_df.unpersist()    cap_lit_req_mm_12_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_3_table")    hive_ctx.sql("DROP TABLE cap_lit_req_mm_12_table")    subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_3_location')])
def calculate_join_5_query(hive_ctx, properties_dict):    cap_data_factor_join_4_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_4_location'))    cap_data_factor_join_4_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_4_df.registerTempTable("cap_data_factor_join_4_table")        cap_web_reg_fl_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_web_reg_fl_df_location'))    cap_web_reg_fl_df.persist(StorageLevel.DISK_ONLY)    cap_web_reg_fl_df.registerTempTable("cap_web_reg_fl_table")        cap_data_factor_join_5_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, b.cap_web_reg_fl                                        from cap_data_factor_join_4_table a full outer join cap_web_reg_fl_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_5_df = hive_ctx.sql(cap_data_factor_join_5_query)    cap_data_factor_join_5_df.write.save(properties_dict.get('demographics_df_5_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_4_df.unpersist()    cap_web_reg_fl_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_4_table")    hive_ctx.sql("DROP TABLE cap_web_reg_fl_table")    subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_4_location')])
def calculate_join_6_query(hive_ctx, properties_dict):    cap_data_factor_join_5_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_5_location'))    cap_data_factor_join_5_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_5_df.registerTempTable("cap_data_factor_join_5_table")        cap_nsghts_nws_prspctvs_pgvw360_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_nsghts_nws_prspctvs_pgvw360_df_location'))    cap_nsghts_nws_prspctvs_pgvw360_df.persist(StorageLevel.DISK_ONLY)    cap_nsghts_nws_prspctvs_pgvw360_df.registerTempTable("cap_Nsghts_Nws_Prspctvs_pgvw360_table")        cap_data_factor_join_6_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, b.cap_Nsghts_Nws_Prspctvs_pgvw360                                        from cap_data_factor_join_5_table a full outer join cap_Nsghts_Nws_Prspctvs_pgvw360_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_6_df = hive_ctx.sql(cap_data_factor_join_6_query)    cap_data_factor_join_6_df.write.save(properties_dict.get('demographics_df_6_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_5_df.unpersist()    cap_nsghts_nws_prspctvs_pgvw360_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_5_table")    hive_ctx.sql("DROP TABLE cap_Nsghts_Nws_Prspctvs_pgvw360_table")    subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_5_location')])
def calculate_join_7_query(hive_ctx, properties_dict):    cap_data_factor_join_6_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_6_location'))    cap_data_factor_join_6_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_6_df.registerTempTable("cap_data_factor_join_6_table")        cap_iras_sess360_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_iras_sess360_df_location'))    cap_iras_sess360_df.persist(StorageLevel.DISK_ONLY)    cap_iras_sess360_df.registerTempTable("cap_IRAs_sess360_table")        cap_data_factor_join_7_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        b.cap_IRAs_sess360                                        from cap_data_factor_join_6_table a full outer join cap_IRAs_sess360_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_7_df = hive_ctx.sql(cap_data_factor_join_7_query)    cap_data_factor_join_7_df.write.save(properties_dict.get('demographics_df_7_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_6_df.unpersist()    cap_iras_sess360_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_6_table")    hive_ctx.sql("DROP TABLE cap_IRAs_sess360_table")        subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_6_location')])
def calculate_join_8_query(hive_ctx, properties_dict):    cap_data_factor_join_7_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_7_location'))    cap_data_factor_join_7_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_7_df.registerTempTable("cap_data_factor_join_7_table")        cap_rim_over_yr_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_rim_over_yr_df_location'))    cap_rim_over_yr_df.persist(StorageLevel.DISK_ONLY)    cap_rim_over_yr_df.registerTempTable("cap_rim_over_yr_table")        cap_data_factor_join_8_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        a.cap_IRAs_sess360, b.cap_rim_over_yr                                        from cap_data_factor_join_7_table a full outer join cap_rim_over_yr_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_8_df = hive_ctx.sql(cap_data_factor_join_8_query)    cap_data_factor_join_8_df.write.save(properties_dict.get('demographics_df_8_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_7_df.unpersist()    cap_rim_over_yr_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_7_table")    hive_ctx.sql("DROP TABLE cap_rim_over_yr_table")        subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_7_location')])
def calculate_join_9_query(hive_ctx, properties_dict):    cap_data_factor_join_8_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_8_location'))    cap_data_factor_join_8_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_8_df.registerTempTable("cap_data_factor_join_8_table")        cap_rim_been_while_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_rim_been_while_df_location'))    cap_rim_been_while_df.persist(StorageLevel.DISK_ONLY)    cap_rim_been_while_df.registerTempTable("cap_rim_been_while_table")        cap_data_factor_join_9_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        a.cap_IRAs_sess360, a.cap_rim_over_yr, b.cap_rim_been_while                                        from cap_data_factor_join_8_table a full outer join cap_rim_been_while_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_9_df = hive_ctx.sql(cap_data_factor_join_9_query)    cap_data_factor_join_9_df.write.save(properties_dict.get('demographics_df_9_location') + '/', format='parquet', mode='overwrite')         cap_data_factor_join_8_df.unpersist()    cap_rim_been_while_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_8_table")    hive_ctx.sql("DROP TABLE cap_rim_been_while_table")        subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_8_location')])
def calculate_join_10_query(hive_ctx, properties_dict):    cap_data_factor_join_9_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_9_location'))    cap_data_factor_join_9_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_9_df.registerTempTable("cap_data_factor_join_9_table")        cap_best_age_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_best_age_df_location'))    cap_best_age_df.persist(StorageLevel.DISK_ONLY)    cap_best_age_df.registerTempTable("cap_best_age_table")        cap_data_factor_join_10_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        a.cap_IRAs_sess360, a.cap_rim_over_yr, a.cap_rim_been_while, b.best_age as cap_best_age                                        from cap_data_factor_join_9_table a full outer join cap_best_age_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_10_df = hive_ctx.sql(cap_data_factor_join_10_query)    cap_data_factor_join_10_df.write.save(properties_dict.get('demographics_df_10_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_9_df.unpersist()    cap_best_age_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_9_table")    hive_ctx.sql("DROP TABLE cap_best_age_table")       subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_9_location')])
def calculate_join_11_query(hive_ctx, properties_dict):    cap_data_factor_join_10_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_10_location'))    cap_data_factor_join_10_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_10_df.registerTempTable("cap_data_factor_join_10_table")        cap_female_fl_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_female_fl_df_location'))    cap_female_fl_df.persist(StorageLevel.DISK_ONLY)    cap_female_fl_df.registerTempTable("cap_female_fl_table")        cap_data_factor_join_11_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        a.cap_IRAs_sess360, a.cap_rim_over_yr, a.cap_rim_been_while, a.cap_best_age, b.cap_female_fl                                        from cap_data_factor_join_10_table a full outer join cap_female_fl_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_11_df = hive_ctx.sql(cap_data_factor_join_11_query)    cap_data_factor_join_11_df.write.save(properties_dict.get('demographics_df_11_location') + '/', format='parquet', mode='overwrite')
    cap_data_factor_join_10_df.unpersist()    cap_female_fl_df.unpersist()    hive_ctx.sql("DROP TABLE cap_data_factor_join_10_table")    hive_ctx.sql("DROP TABLE cap_female_fl_table")       subprocess.call(['hdfs', 'dfs', '-rm', '-r', properties_dict.get('demographics_df_10_location')])
def calculate_join_12_query(hive_ctx, properties_dict):    cap_data_factor_join_11_df = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_11_location'))    cap_data_factor_join_11_df.persist(StorageLevel.DISK_ONLY)    cap_data_factor_join_11_df.registerTempTable("cap_data_factor_join_11_table")        cap_single_fl_df = hive_ctx.read.format('parquet').load(properties_dict.get('cap_single_fl_df_location'))    cap_single_fl_df.persist(StorageLevel.DISK_ONLY)    cap_single_fl_df.registerTempTable("cap_single_fl_table")        cap_data_factor_join_12_query =  """ select coalesce(a.po_id , b.po_id) as po_id, a.cap_educ_9514, a.cap_inc_fl_higher_rngs_8201, a.cap_contact_cnt_06,                                        a.cap_watch_list_fl, a.cap_lit_req_mm_12, a.cap_web_reg_fl, a.cap_Nsghts_Nws_Prspctvs_pgvw360,                                        a.cap_IRAs_sess360, a.cap_rim_over_yr, a.cap_rim_been_while, a.cap_best_age, a.cap_female_fl, b.cap_single_fl                                        from cap_data_factor_join_11_table a full outer join cap_single_fl_table b                                       on cast(a.po_id as int) = cast(b.po_id as int)"""        cap_data_factor_join_12_df = hive_ctx.sql(cap_data_factor_join_12_query)    cap_data_factor_join_12_df.write.save(properties_dict.get('demographics_df_12_location') + '/', format='parquet', mode='overwrite')        cap_data_factor_join_11_df.unpersist()    cap_single_fl_df.unpersist()
def calculate_demographics_data_factor(hive_context, properties_dict):    run_demographic_datafactors(hive_context, properties_dict)    calculate_join_1_query(hive_context, properties_dict)    calculate_join_2_query(hive_context, properties_dict)    calculate_join_3_query(hive_context, properties_dict)    calculate_join_4_query(hive_context, properties_dict)    calculate_join_5_query(hive_context, properties_dict)    calculate_join_6_query(hive_context, properties_dict)    calculate_join_7_query(hive_context, properties_dict)    calculate_join_8_query(hive_context, properties_dict)    calculate_join_9_query(hive_context, properties_dict)    calculate_join_10_query(hive_context, properties_dict)    calculate_join_11_query(hive_context, properties_dict)    calculate_join_12_query(hive_context, properties_dict)
if __name__ == "__main__":    conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict( sys.argv[1:] )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'debugutils.py' )     import sparkutils    import debugutils        DEBUG = properties_dict.get( 'debug_flag' )    sparkutils.quiet_logs(sc)    sparkutils.addallpyfiles(sc)            hive_ctx=HiveContext(sc)        calculate_demographics_data_factor(hive_ctx, properties_dict)    demographics_data_factor_final = hive_ctx.read.format('parquet').load(properties_dict.get('demographics_df_12_location'))    demographics_data_factor_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile( properties_dict.get( 'data_item_dir' ) + "/" + properties_dict.get( 'workflow_id' ) + "/" + "cap_data_factor_demographic" )        sc.stop()

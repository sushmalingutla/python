lgacy_brkg_lmv_bal_am_df_location=/data/temp/lgacy_brkg_lmv_bal_am
lgacy_mf_sbs_bal_am_df_location=/data/temp/lgacy_mf_sbs_bal_am
lgacy_mf_nsbs_bal_am_df_location=/data/temp/lgacy_mf_nsbs_bal_am
vba_mf_sbs_bal_am_df_location=/data/temp/vba_mf_sbs_bal_am
vba_mf_nsbs_bal_am_df_location=/data/temp/vba_mf_nsbs_bal_am
vba_brkg_lmv_bal_am_df_location=/data/temp/vba_brkg_lmv_bal_am

""" This module will combine the dataframes created by the other complex balances functions. There are seven:    1. Complex balance for Annuities    2. Complex balance for Legacy Brokerage    3. Complex balance for VBA Brokerage    4. Complex balance for Legacy Mutual Fund SBS    5. Complex balance for VBA Mutual Fund SBS     6. Complex balance for Legacy Mutual Fund NSBS     7. Complex balance for VBA Mutual Fund NSBS      
 This module has the following function(s) - union_new_df: This method will declare the next dataframe as a temporary table and then execute the SQL to  Union the new temp table to the existing one. So this will get called 7 times, passing in the old and new   dataframe names.    create_complex_bal_am_final:  This method will call the union_new_df method to create the new dataframe.      """from pyspark import SparkConf, SparkContext,SQLContext,Row,HiveContext,StorageLevelimport sysimport unittestimport subprocessimport os APP_NAME="complex_bal_am Data Item pyspark Application"import debugutilsimport properties_readerproperties_dict = properties_reader.get_properties_dict(sys.argv[1:])    DEBUG = debugutils.is_debug(properties_dict)   
def union_new_df ( HiveCtx, old_df, new_df) :      """This is the method which declares the temp tables and unions the dataframes          Input:  properties_dict, hive context, the name of the original dataframe and the name of the new dataframe         Output: new resulting dataframe     """       import debugutils        old_df.registerTempTable("old_table")     new_df.registerTempTable("new_table")         union_query = """SELECT DISTINCT a.po_id, sum(a.complex_bal_am) as complex_bal_am                     FROM(SELECT * from new_table UNION ALL SELECT * from old_table) a group by a.po_id"""    import datetime    print "Start time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print union_query                              union_df = HiveCtx.sql(union_query)    print "End time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print "############################union_query completed###################################"          if DEBUG:         debugutils.debug_query_info("union_query",union_query,union_df)                                                                                          return union_df 
def calculate_complex_bal_am_not_equal_to_zero ( HiveCtx, properties_dict ) :        """This is the method which declares the temp tables and unions the dataframes.         Input:  properties_dict, hive context, the name of the original dataframe and the name of the new dataframe         Output: new resulting dataframe with complex_bal_am <>0    """       import debugutils        complex_bal_am_df = calculate_complex_bal_am( HiveCtx, properties_dict)    complex_bal_am_df.registerTempTable("complex_bal_am_df_table")         complex_bal_am_not_equal_to_zero_query = """SELECT DISTINCT a.po_id, sum(a.complex_bal_am) as complex_bal_am FROM complex_bal_am_df_table a group by a.po_id having complex_bal_am <> 0"""                                          complex_bal_am_not_equal_to_zero_df = HiveCtx.sql(complex_bal_am_not_equal_to_zero_query)         if DEBUG:         debugutils.debug_query_info("complex_bal_am_not_equal_to_zero",complex_bal_am_not_equal_to_zero_query,complex_bal_am_not_equal_to_zero_df)                                                                                          return complex_bal_am_not_equal_to_zero_df
def calculate_complex_bal_am(HiveCtx, properties_dict) :     """This is the method which calls the union_new_df dataframe after running the methods to create the dataframes       to be used.           Input:  HiveCtx, properties_dict to create HiveCtx and call the methods to create the dataframes         Output: complex_bal_am_final dataframe     """    import complex_annuity    import lgacy_brkg_lmv_bal_am    import vba_brkg_lmv_bal_am    import lgacy_mf_sbs_bal_am    import vba_mf_sbs_bal_am    import lgacy_mf_nsbs_bal_am    import vba_mf_nsbs_bal_am              global DEBUG        import debugutils        DEBUG = debugutils.is_debug(properties_dict)   
    annty_bal_df = HiveCtx.sql("SELECT * FROM ANNTY_BAL_AM_TB")      annty_bal_df = annty_bal_df.withColumnRenamed('complex_assets','complex_bal_am')              lgacy_brkg_lmv_bal_am_df = HiveCtx.sql("SELECT * FROM LGACY_BRKG_LMV_BAL_AM_TB")         lgacy_brkg_lmv_bal_am_df = lgacy_brkg_lmv_bal_am_df.withColumnRenamed('lgacy_brkg_lmv_bal_am','complex_bal_am')         complex_bal_am_df = union_new_df( HiveCtx, annty_bal_df, lgacy_brkg_lmv_bal_am_df)        vba_brkg_lmv_bal_am_df = HiveCtx.sql("SELECT * FROM VBA_BRKG_LMV_BAL_AM_TB")      vba_brkg_lmv_bal_am_df = vba_brkg_lmv_bal_am_df.withColumnRenamed('vba_brkg_lmv_bal_am','complex_bal_am')             complex_bal_am_df_1 = union_new_df( HiveCtx, complex_bal_am_df, vba_brkg_lmv_bal_am_df)
    lgacy_mf_sbs_bal_am_df = lgacy_mf_sbs_bal_am.calculate_lgacy_mf_sbs_bal_am(HiveCtx, properties_dict)      lgacy_mf_sbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)          HiveCtx.registerDataFrameAsTable(lgacy_mf_sbs_bal_am_df, "LGACY_MF_SBS_BAL_AM_TB")      lgacy_mf_sbs_bal_am_df = lgacy_mf_sbs_bal_am_df.withColumnRenamed('lgacy_mf_sbs_bal_am','complex_bal_am')              complex_bal_am_df_2 = union_new_df( HiveCtx, complex_bal_am_df_1, lgacy_mf_sbs_bal_am_df)         vba_mf_sbs_bal_am_df = vba_mf_sbs_bal_am.calculate_vba_mf_sbs_bal_am(HiveCtx, properties_dict)     vba_mf_sbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)        HiveCtx.registerDataFrameAsTable(vba_mf_sbs_bal_am_df, "VBA_MF_SBS_BAL_AM_TB")      vba_mf_sbs_bal_am_df = vba_mf_sbs_bal_am_df.withColumnRenamed('vba_mf_sbs_bal_am','complex_bal_am')           complex_bal_am_df_3 = union_new_df( HiveCtx, complex_bal_am_df_2, vba_mf_sbs_bal_am_df)        lgacy_mf_nsbs_bal_am_df = lgacy_mf_nsbs_bal_am.calculate_lgacy_mf_nsbs_bal_am(HiveCtx, properties_dict)     lgacy_mf_nsbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)          HiveCtx.registerDataFrameAsTable(lgacy_mf_nsbs_bal_am_df, "LGACY_MF_NSBS_BAL_AM_TB")      lgacy_mf_nsbs_bal_am_df = lgacy_mf_nsbs_bal_am_df.withColumnRenamed('lgacy_mf_nsbs_bal_am','complex_bal_am')              complex_bal_am_df_4 = union_new_df( HiveCtx, complex_bal_am_df_3, lgacy_mf_nsbs_bal_am_df)        vba_mf_nsbs_bal_am_df = vba_mf_nsbs_bal_am.calculate_vba_mf_nsbs_bal_am(HiveCtx, properties_dict)       vba_mf_nsbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)        HiveCtx.registerDataFrameAsTable(vba_mf_nsbs_bal_am_df, "VBA_MF_NSBS_BAL_AM_TB")           vba_mf_nsbs_bal_am_df = vba_mf_nsbs_bal_am_df.withColumnRenamed('vba_mf_nsbs_bal_am','complex_bal_am')              final_complex_bal_am_df = union_new_df( HiveCtx, complex_bal_am_df_4, vba_mf_nsbs_bal_am_df)        HiveCtx.sql("DROP TABLE ANNTY_BAL_AM_TB")      HiveCtx.sql("DROP TABLE LGACY_BRKG_LMV_BAL_AM_TB")     HiveCtx.sql("DROP TABLE VBA_BRKG_LMV_BAL_AM_TB")     lgacy_mf_sbs_bal_am_df.unpersist()    vba_mf_sbs_bal_am_df.unpersist()    lgacy_mf_nsbs_bal_am_df.unpersist()    vba_mf_nsbs_bal_am_df.unpersist()                                                    return final_complex_bal_am_df if __name__ == "__main__":    conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict( sys.argv[1:] )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )            import sparkutils    sparkutils.quiet_logs(sc)    sparkutils.addAllPyFiles(sc)        HiveCtx=HiveContext(sc)       calculate_complex_bal_am_final = calculate_complex_bal_am(HiveCtx, properties_dict )    calculate_complex_bal_am_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile(properties_dict.get( 'data_item_dir' ) + "/" + properties_dict.get( 'workflow_id' ) + "/" + "complex_bal_am" )        sc.stop()  
    
    
    
""" The lgacy_brkg_lmv_bal_am module will calculate the Legacy Brokerage Balances for accounts against each clients. 
This module has the following function(s)-
calculate_lgacy_brkg_lmv_bal_am: This method will calculate legacy brokerage balances.               """from pyspark import SparkConf, SparkContext,SQLContext,Row,HiveContext,StorageLevelimport hashlibimport timeimport sysimport datetime  APP_NAME="lgacy_brkg_lmv_bal_am Data Item pyspark Application"
def calculate_lgacy_brkg_lmv_bal_am(HiveCtx,properties_dict):        """ calculate_lgacy_brkg_lmv_bal_am: This method will calculate the Legacy Brokerage Balances for accounts against each clients.        Input:sparkcontext,properties_dict        Output:Data Frame with (po_id,lgacy_brkg_lmv_bal_am)"""      import debugutils        DEBUG = debugutils.is_debug( properties_dict )                                                      lgacy_brkg_lmv_bal_am_query = """SELECT    f.po_id AS po_id,                                                 SUM(g.mm121_end_bal_am) AS lgacy_brkg_lmv_bal_am                                FROM       {0} a                                          ,{1} b                                          ,{2} c                                          ,{3} d                                          ,{4} e                                          ,{5} f                                          ,{6} g                                          ,{7} h                                                                        WHERE      a.serv_id = 9                                AND        from_unixtime(unix_timestamp(a.efftv_end_dt , 'yyyy-MM-dd'), 'yyyy-MM-dd') = '9999-12-31'                                AND        b.rlshp_id = g.acct_id                                AND        a.sag_id = b.sag_id                                AND        c.acct_id = d.rlshp_id                                AND        b.rlshp_typ_cd = 'ACCT'                                AND        c.sag_id = a.sag_id                                AND        d.sag_id = e.sag_id                                AND        f.po_role_cd = 'PRRT'                                AND        e.serv_id IN (8,38)                                                                AND        e.sag_id = f.sag_id                                AND        h.prtn_id = g.prtn_id                                AND        g.mm121_end_bal_am <> 0                                                                AND        g.seq_no = 1                                AND        h.actv_fl = 'Y'                                    GROUP BY f.po_id                                                            """.format(properties_dict.get( 'tsag_tableName' ),   \                                    properties_dict.get( 'tsag_acct_rlshp_tableName' ),                  \                                    properties_dict.get( 'tent_sag_pos_rlshp_tableName' ) ,properties_dict.get( 'tsag_acct_rlshp_tableName' ),                  \                                    properties_dict.get( 'tsag_tableName' ) ,properties_dict.get( 'tsag_bus_rlshp_tableName' ),                    \                                    properties_dict.get( 'tbt_vbs_amt_tableName' ) ,properties_dict.get( 'tmm_end_bal_prtn_tableName' ))                                                import datetime    print "Start time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print lgacy_brkg_lmv_bal_am_query                                                lgacy_brkg_lmv_bal_am_df=HiveCtx.sql(lgacy_brkg_lmv_bal_am_query)    print "End time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print "############################lgacy_brkg_lmv_bal_am_query completed###################################"    lgacy_brkg_lmv_bal_am_df.persist(StorageLevel.DISK_ONLY)    HiveCtx.registerDataFrameAsTable(lgacy_brkg_lmv_bal_am_df, "LGACY_BRKG_LMV_BAL_AM_TB")    #lgacy_brkg_lmv_bal_am_df.persist(StorageLevel.DISK_ONLY)        if DEBUG:        debugutils.debug_query_info("lgacy_brkg_lmv_bal_am_df",lgacy_brkg_lmv_bal_am_query,lgacy_brkg_lmv_bal_am_df)    #lgacy_brkg_lmv_bal_am_df.unpersist()    #lgacy_brkg_lmv_bal_am_df.write.save(properties_dict.get('lgacy_brkg_lmv_bal_am_df_location') + '/', format='parquet', mode='overwrite')                 return lgacy_brkg_lmv_bal_am_df

if __name__ == "__main__":           conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict( sys.argv[1:] )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'debugutils.py' )     DEBUG = properties_dict.get( 'debug_flag' )    import sparkutils    sparkutils.quiet_logs(sc)    sparkutils.addAllPyFiles(sc)    import debugutils        HiveCtx = HiveContext(sc)     calculate_lgacy_brkg_lmv_bal_am_final = calculate_lgacy_brkg_lmv_bal_am( HiveCtx, properties_dict )    calculate_lgacy_brkg_lmv_bal_am_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile( properties_dict.get( 'data_item_dir' ) + "/" + properties_dict.get( 'workflow_id' ) + "/" + "lgacy_brkg_lmv_bal_am" )        sc.stop()
        

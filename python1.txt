lgacy_brkg_lmv_bal_am_df_location=/data/temp/lgacy_brkg_lmv_bal_am
lgacy_mf_sbs_bal_am_df_location=/data/temp/lgacy_mf_sbs_bal_am
lgacy_mf_nsbs_bal_am_df_location=/data/temp/lgacy_mf_nsbs_bal_am
vba_mf_sbs_bal_am_df_location=/data/temp/vba_mf_sbs_bal_am
vba_mf_nsbs_bal_am_df_location=/data/temp/vba_mf_nsbs_bal_am
vba_brkg_lmv_bal_am_df_location=/data/temp/vba_brkg_lmv_bal_am

""" This module will combine the dataframes created by the other complex balances functions. There are seven:    1. Complex balance for Annuities    2. Complex balance for Legacy Brokerage    3. Complex balance for VBA Brokerage    4. Complex balance for Legacy Mutual Fund SBS    5. Complex balance for VBA Mutual Fund SBS     6. Complex balance for Legacy Mutual Fund NSBS     7. Complex balance for VBA Mutual Fund NSBS      
 This module has the following function(s) - union_new_df: This method will declare the next dataframe as a temporary table and then execute the SQL to  Union the new temp table to the existing one. So this will get called 7 times, passing in the old and new   dataframe names.    create_complex_bal_am_final:  This method will call the union_new_df method to create the new dataframe.      """from pyspark import SparkConf, SparkContext,SQLContext,Row,HiveContext,StorageLevelimport sysimport unittestimport subprocessimport os APP_NAME="complex_bal_am Data Item pyspark Application"import debugutilsimport properties_readerproperties_dict = properties_reader.get_properties_dict(sys.argv[1:])    DEBUG = debugutils.is_debug(properties_dict)   
def union_new_df ( HiveCtx, old_df, new_df) :      """This is the method which declares the temp tables and unions the dataframes          Input:  properties_dict, hive context, the name of the original dataframe and the name of the new dataframe         Output: new resulting dataframe     """       import debugutils        old_df.registerTempTable("old_table")     new_df.registerTempTable("new_table")         union_query = """SELECT DISTINCT a.po_id, sum(a.complex_bal_am) as complex_bal_am                     FROM(SELECT * from new_table UNION ALL SELECT * from old_table) a group by a.po_id"""    import datetime    print "Start time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print union_query                              union_df = HiveCtx.sql(union_query)    print "End time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print "############################union_query completed###################################"          if DEBUG:         debugutils.debug_query_info("union_query",union_query,union_df)                                                                                          return union_df 
def calculate_complex_bal_am_not_equal_to_zero ( HiveCtx, properties_dict ) :        """This is the method which declares the temp tables and unions the dataframes.         Input:  properties_dict, hive context, the name of the original dataframe and the name of the new dataframe         Output: new resulting dataframe with complex_bal_am <>0    """       import debugutils        complex_bal_am_df = calculate_complex_bal_am( HiveCtx, properties_dict)    complex_bal_am_df.registerTempTable("complex_bal_am_df_table")         complex_bal_am_not_equal_to_zero_query = """SELECT DISTINCT a.po_id, sum(a.complex_bal_am) as complex_bal_am FROM complex_bal_am_df_table a group by a.po_id having complex_bal_am <> 0"""                                          complex_bal_am_not_equal_to_zero_df = HiveCtx.sql(complex_bal_am_not_equal_to_zero_query)         if DEBUG:         debugutils.debug_query_info("complex_bal_am_not_equal_to_zero",complex_bal_am_not_equal_to_zero_query,complex_bal_am_not_equal_to_zero_df)                                                                                          return complex_bal_am_not_equal_to_zero_df
def calculate_complex_bal_am(HiveCtx, properties_dict) :     """This is the method which calls the union_new_df dataframe after running the methods to create the dataframes       to be used.           Input:  HiveCtx, properties_dict to create HiveCtx and call the methods to create the dataframes         Output: complex_bal_am_final dataframe     """    import complex_annuity    import lgacy_brkg_lmv_bal_am    import vba_brkg_lmv_bal_am    import lgacy_mf_sbs_bal_am    import vba_mf_sbs_bal_am    import lgacy_mf_nsbs_bal_am    import vba_mf_nsbs_bal_am              global DEBUG        import debugutils        DEBUG = debugutils.is_debug(properties_dict)   
    annty_bal_df = HiveCtx.sql("SELECT * FROM ANNTY_BAL_AM_TB")      annty_bal_df = annty_bal_df.withColumnRenamed('complex_assets','complex_bal_am')              lgacy_brkg_lmv_bal_am_df = HiveCtx.sql("SELECT * FROM LGACY_BRKG_LMV_BAL_AM_TB")         lgacy_brkg_lmv_bal_am_df = lgacy_brkg_lmv_bal_am_df.withColumnRenamed('lgacy_brkg_lmv_bal_am','complex_bal_am')         complex_bal_am_df = union_new_df( HiveCtx, annty_bal_df, lgacy_brkg_lmv_bal_am_df)        vba_brkg_lmv_bal_am_df = HiveCtx.sql("SELECT * FROM VBA_BRKG_LMV_BAL_AM_TB")      vba_brkg_lmv_bal_am_df = vba_brkg_lmv_bal_am_df.withColumnRenamed('vba_brkg_lmv_bal_am','complex_bal_am')             complex_bal_am_df_1 = union_new_df( HiveCtx, complex_bal_am_df, vba_brkg_lmv_bal_am_df)
    lgacy_mf_sbs_bal_am_df = lgacy_mf_sbs_bal_am.calculate_lgacy_mf_sbs_bal_am(HiveCtx, properties_dict)      lgacy_mf_sbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)          HiveCtx.registerDataFrameAsTable(lgacy_mf_sbs_bal_am_df, "LGACY_MF_SBS_BAL_AM_TB")      lgacy_mf_sbs_bal_am_df = lgacy_mf_sbs_bal_am_df.withColumnRenamed('lgacy_mf_sbs_bal_am','complex_bal_am')              complex_bal_am_df_2 = union_new_df( HiveCtx, complex_bal_am_df_1, lgacy_mf_sbs_bal_am_df)         vba_mf_sbs_bal_am_df = vba_mf_sbs_bal_am.calculate_vba_mf_sbs_bal_am(HiveCtx, properties_dict)     vba_mf_sbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)        HiveCtx.registerDataFrameAsTable(vba_mf_sbs_bal_am_df, "VBA_MF_SBS_BAL_AM_TB")      vba_mf_sbs_bal_am_df = vba_mf_sbs_bal_am_df.withColumnRenamed('vba_mf_sbs_bal_am','complex_bal_am')           complex_bal_am_df_3 = union_new_df( HiveCtx, complex_bal_am_df_2, vba_mf_sbs_bal_am_df)        lgacy_mf_nsbs_bal_am_df = lgacy_mf_nsbs_bal_am.calculate_lgacy_mf_nsbs_bal_am(HiveCtx, properties_dict)     lgacy_mf_nsbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)          HiveCtx.registerDataFrameAsTable(lgacy_mf_nsbs_bal_am_df, "LGACY_MF_NSBS_BAL_AM_TB")      lgacy_mf_nsbs_bal_am_df = lgacy_mf_nsbs_bal_am_df.withColumnRenamed('lgacy_mf_nsbs_bal_am','complex_bal_am')              complex_bal_am_df_4 = union_new_df( HiveCtx, complex_bal_am_df_3, lgacy_mf_nsbs_bal_am_df)        vba_mf_nsbs_bal_am_df = vba_mf_nsbs_bal_am.calculate_vba_mf_nsbs_bal_am(HiveCtx, properties_dict)       vba_mf_nsbs_bal_am_df.persist(StorageLevel.MEMORY_AND_DISK)        HiveCtx.registerDataFrameAsTable(vba_mf_nsbs_bal_am_df, "VBA_MF_NSBS_BAL_AM_TB")           vba_mf_nsbs_bal_am_df = vba_mf_nsbs_bal_am_df.withColumnRenamed('vba_mf_nsbs_bal_am','complex_bal_am')              final_complex_bal_am_df = union_new_df( HiveCtx, complex_bal_am_df_4, vba_mf_nsbs_bal_am_df)        HiveCtx.sql("DROP TABLE ANNTY_BAL_AM_TB")      HiveCtx.sql("DROP TABLE LGACY_BRKG_LMV_BAL_AM_TB")     HiveCtx.sql("DROP TABLE VBA_BRKG_LMV_BAL_AM_TB")     lgacy_mf_sbs_bal_am_df.unpersist()    vba_mf_sbs_bal_am_df.unpersist()    lgacy_mf_nsbs_bal_am_df.unpersist()    vba_mf_nsbs_bal_am_df.unpersist()                                                    return final_complex_bal_am_df if __name__ == "__main__":    conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict( sys.argv[1:] )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )            import sparkutils    sparkutils.quiet_logs(sc)    sparkutils.addAllPyFiles(sc)        HiveCtx=HiveContext(sc)       calculate_complex_bal_am_final = calculate_complex_bal_am(HiveCtx, properties_dict )    calculate_complex_bal_am_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile(properties_dict.get( 'data_item_dir' ) + "/" + properties_dict.get( 'workflow_id' ) + "/" + "complex_bal_am" )        sc.stop()  
    
    
    
""" The lgacy_brkg_lmv_bal_am module will calculate the Legacy Brokerage Balances for accounts against each clients. 
This module has the following function(s)-
calculate_lgacy_brkg_lmv_bal_am: This method will calculate legacy brokerage balances.               """from pyspark import SparkConf, SparkContext,SQLContext,Row,HiveContext,StorageLevelimport hashlibimport timeimport sysimport datetime  APP_NAME="lgacy_brkg_lmv_bal_am Data Item pyspark Application"
def calculate_lgacy_brkg_lmv_bal_am(HiveCtx,properties_dict):        """ calculate_lgacy_brkg_lmv_bal_am: This method will calculate the Legacy Brokerage Balances for accounts against each clients.        Input:sparkcontext,properties_dict        Output:Data Frame with (po_id,lgacy_brkg_lmv_bal_am)"""      import debugutils        DEBUG = debugutils.is_debug( properties_dict )                                                      lgacy_brkg_lmv_bal_am_query = """SELECT    f.po_id AS po_id,                                                 SUM(g.mm121_end_bal_am) AS lgacy_brkg_lmv_bal_am                                FROM       {0} a                                          ,{1} b                                          ,{2} c                                          ,{3} d                                          ,{4} e                                          ,{5} f                                          ,{6} g                                          ,{7} h                                                                        WHERE      a.serv_id = 9                                AND        from_unixtime(unix_timestamp(a.efftv_end_dt , 'yyyy-MM-dd'), 'yyyy-MM-dd') = '9999-12-31'                                AND        b.rlshp_id = g.acct_id                                AND        a.sag_id = b.sag_id                                AND        c.acct_id = d.rlshp_id                                AND        b.rlshp_typ_cd = 'ACCT'                                AND        c.sag_id = a.sag_id                                AND        d.sag_id = e.sag_id                                AND        f.po_role_cd = 'PRRT'                                AND        e.serv_id IN (8,38)                                                                AND        e.sag_id = f.sag_id                                AND        h.prtn_id = g.prtn_id                                AND        g.mm121_end_bal_am <> 0                                                                AND        g.seq_no = 1                                AND        h.actv_fl = 'Y'                                    GROUP BY f.po_id                                                            """.format(properties_dict.get( 'tsag_tableName' ),   \                                    properties_dict.get( 'tsag_acct_rlshp_tableName' ),                  \                                    properties_dict.get( 'tent_sag_pos_rlshp_tableName' ) ,properties_dict.get( 'tsag_acct_rlshp_tableName' ),                  \                                    properties_dict.get( 'tsag_tableName' ) ,properties_dict.get( 'tsag_bus_rlshp_tableName' ),                    \                                    properties_dict.get( 'tbt_vbs_amt_tableName' ) ,properties_dict.get( 'tmm_end_bal_prtn_tableName' ))                                                import datetime    print "Start time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print lgacy_brkg_lmv_bal_am_query                                                lgacy_brkg_lmv_bal_am_df=HiveCtx.sql(lgacy_brkg_lmv_bal_am_query)    print "End time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print "############################lgacy_brkg_lmv_bal_am_query completed###################################"    lgacy_brkg_lmv_bal_am_df.persist(StorageLevel.DISK_ONLY)    HiveCtx.registerDataFrameAsTable(lgacy_brkg_lmv_bal_am_df, "LGACY_BRKG_LMV_BAL_AM_TB")    #lgacy_brkg_lmv_bal_am_df.persist(StorageLevel.DISK_ONLY)        if DEBUG:        debugutils.debug_query_info("lgacy_brkg_lmv_bal_am_df",lgacy_brkg_lmv_bal_am_query,lgacy_brkg_lmv_bal_am_df)    #lgacy_brkg_lmv_bal_am_df.unpersist()    #lgacy_brkg_lmv_bal_am_df.write.save(properties_dict.get('lgacy_brkg_lmv_bal_am_df_location') + '/', format='parquet', mode='overwrite')                 return lgacy_brkg_lmv_bal_am_df

if __name__ == "__main__":           conf = SparkConf().setAppName(APP_NAME)    sc   = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict( sys.argv[1:] )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'sparkutils.py' )    sc.addPyFile( properties_dict.get( 'utility_path' ) + 'debugutils.py' )     DEBUG = properties_dict.get( 'debug_flag' )    import sparkutils    sparkutils.quiet_logs(sc)    sparkutils.addAllPyFiles(sc)    import debugutils        HiveCtx = HiveContext(sc)     calculate_lgacy_brkg_lmv_bal_am_final = calculate_lgacy_brkg_lmv_bal_am( HiveCtx, properties_dict )    calculate_lgacy_brkg_lmv_bal_am_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile( properties_dict.get( 'data_item_dir' ) + "/" + properties_dict.get( 'workflow_id' ) + "/" + "lgacy_brkg_lmv_bal_am" )        sc.stop()
        
from pyspark import SparkConf, SparkContext, SQLContext, Row, HiveContext,StorageLevelimport hashlibimport timeimport sysimport datetime
APP_NAME = "wc_join_data Data Item pyspark Application"
def calculate_wc_join_data(HiveCtx, properties_dict):
    """ calculate_wc_join_data: This method will calculate wc join data.        Input:sparkcontext,properties_dict        Output:Data Frame with (po_id,calculate_wc_join_data)"""    import debugutils    DEBUG = debugutils.is_debug(properties_dict)
#    import best_age    import debugutils
    VS_VSB_V24_q1 =  """SELECT VS.vgi_clnt_id        FROM entmaster.tsag VS        JOIN entmaster.tsag_bus_rlshp VSB ON (VSB.sag_id=VS.sag_id AND VS.efftv_end_dt = '9999-12-31')        JOIN entmaster.tcin024 V24 ON (V24.vgi_clnt_id=VS.vgi_clnt_id AND  V24.DECSD_FL <> 'Y') """
    VS_VSB_V24_df = HiveCtx.sql(VS_VSB_V24_q1)    VS_VSB_V24_df.registerTempTable("VS_VSB_V24_tb")    #print "COUNT OF VS_VSB_V24_df = " + str(VS_VSB_V24_df.count())    debugutils.spark_logs("VS_VSB_V24_q1",HiveCtx)
    WC_q2 = """SELECT V22.vgi_clnt_id AS po_id, V22.zip_cd, V22.zip_plus_4_cd,BA.best_age            FROM entmaster.tcin022  V22            JOIN VS_VSB_V24_tb ON (VS_VSB_V24_tb.vgi_clnt_id=V22.vgi_clnt_id)            JOIN BEST_AGE_TB BA ON (BA.po_id=V22.vgi_clnt_id AND VS_VSB_V24_tb.vgi_clnt_id=BA.po_id)"""
    WC_df = HiveCtx.sql(WC_q2)    WC_df.write.save(properties_dict.get('wc_df_location') + '/', format='parquet', mode='overwrite')    #WC_df.registerTempTable("WC_tb")    debugutils.spark_logs("WC_q2",HiveCtx)    #print "COUNT OF WC_df = " + str(WC_df.count())
    ixi_mean_ebs_write_q1 = """SELECT wc_tot_asset, wc_assetmix_mutl_fnd,zip_cd,zip_plus_4_cd,age_cd from exdmomaster.twealth_complete"""    print ixi_mean_ebs_write_q1    ixi_mean_ebs_df = HiveCtx.sql(ixi_mean_ebs_write_q1)    ixi_mean_ebs_df.write.save(properties_dict.get('ixi_mean_ebs_location') + '/', format='parquet', mode='overwrite')    #ixi_mean_ebs_write_df.registerTempTable("ixi_mean_ebs_tb")    ixi_mean_ebs_df = HiveCtx.read.format('parquet').load(properties_dict.get('ixi_mean_ebs_location'))    #ixi_mean_ebs_df.persist(StorageLevel.DISK_ONLY)    ixi_mean_ebs_df.registerTempTable("ixi_mean_ebs_tb")    debugutils.spark_logs("ixi_mean_ebs_write_q1",HiveCtx)    #print "ixi_mean_ebs_df = " + str(ixi_mean_ebs_df.count())
    WC_EBS_df = HiveCtx.read.format('parquet').load(properties_dict.get('wc_df_location'))    WC_EBS_df.persist(StorageLevel.DISK_ONLY)    WC_EBS_df.registerTempTable("WC_tb")    #print "COUNT OF WC_df = " + str(WC_EBS_df.count())
    C_WC_q2 = """SELECT c.wc_tot_asset, c.wc_assetmix_mutl_fnd,c.zip_cd,c.zip_plus_4_cd        FROM WC_tb        JOIN ixi_mean_ebs_tb AS c ON (c.zip_cd = WC_tb.zip_cd  AND c.zip_plus_4_cd = WC_tb.zip_plus_4_cd AND WC_tb.best_age = C.age_cd) """
    C_WC_df = HiveCtx.sql(C_WC_q2)    C_WC_df.registerTempTable("C_WC_tb")    debugutils.spark_logs("C_WC_q2",HiveCtx)    #print "COUNT OF C_WC_df = " + str(C_WC_df.count())
    ixi_max_ebs_write_q1 = """SELECT wc_tot_asset_max, wc_assetmix_mutl_fnd_max,zip_cd,zip_plus_4_cd,age_cd from exdmomaster.twealth_complete_max"""    print ixi_max_ebs_write_q1    ixi_max_ebs_df = HiveCtx.sql(ixi_max_ebs_write_q1)    ixi_max_ebs_df.write.save(properties_dict.get('ixi_max_ebs_location') + '/', format='parquet', mode='overwrite')    #ixi_mean_ebs_write_df.registerTempTable("ixi_mean_ebs_tb")    ixi_max_ebs_df = HiveCtx.read.format('parquet').load(properties_dict.get('ixi_max_ebs_location'))    #ixi_max_ebs_df.persist(StorageLevel.DISK_ONLY)    ixi_max_ebs_df.registerTempTable("ixi_max_ebs_tb")    debugutils.spark_logs("ixi_max_ebs_write_q1",HiveCtx)    #print "ixi_max_ebs_df = " + str(ixi_max_ebs_df.count())
    D_WC_q2 = """SELECT d.wc_tot_asset_max, d.wc_assetmix_mutl_fnd_max,d.zip_cd,d.zip_plus_4_cd        FROM ixi_max_ebs_tb AS d        JOIN WC_tb ON (d.zip_cd = WC_tb.zip_cd  AND d.zip_plus_4_cd = WC_tb.zip_plus_4_cd AND WC_tb.best_age = d.age_cd) """
    D_WC_df = HiveCtx.sql(D_WC_q2)    debugutils.spark_logs("D_WC_df",HiveCtx)    D_WC_df.registerTempTable("D_WC_tb")    #print "COUNT OF D_WC_df = " + str(D_WC_df.count())
    wc_join_data_query = """SELECT V22.po_id AS po_id, V22.best_age AS age_cd,C.wc_tot_asset, C.wc_assetmix_mutl_fnd, D.wc_tot_asset_max, D.wc_assetmix_mutl_fnd_max        FROM VS_VSB_V24_tb VS        JOIN WC_tb V22 ON (V22.po_id = VS.vgi_clnt_id)        JOIN C_WC_tb C ON (V22.zip_cd = C.zip_cd AND V22.zip_plus_4_cd = C.zip_plus_4_cd)        JOIN D_WC_tb D ON (V22.zip_cd = D.zip_cd AND V22.zip_plus_4_cd = D.zip_plus_4_cd) """

    import datetime    print "Start time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    print wc_join_data_query    wc_join_data_df = HiveCtx.sql(wc_join_data_query).repartition(500,"po_id")    debugutils.spark_logs("wc_join_data_query",HiveCtx)    print "End time : " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")    #print "############################wc_join_data_query completed###################################"
    HiveCtx.sql("DROP TABLE BEST_AGE_TB")
    if DEBUG:        debugutils.debug_query_info("wc_join_data_df", wc_join_data_query, wc_join_data_df)
    wc_join_data_df.write.save(properties_dict.get('wc_join_data_df_location') + '/', format='parquet', mode='overwrite')    ixi_mean_ebs_df.unpersist()    WC_df.unpersist()    C_WC_df.unpersist()    D_WC_df.unpersist()    ixi_max_ebs_df.unpersist()
    print "############################wc_join_data_query completed###################################"
   #wc_join_data_df.write.save(properties_dict.get('wc_join_data_df_location') + '/', format='parquet', mode='overwrite')
    return wc_join_data_df

if __name__ == "__main__":
    conf = SparkConf().setAppName(APP_NAME)    sc = SparkContext(conf=conf)    import properties_reader    properties_dict = properties_reader.get_properties_dict(sys.argv[1:])    sc.addPyFile(properties_dict.get('utility_path') + 'sparkutils.py')    sc.addPyFile(properties_dict.get('utility_path') + 'debugutils.py')    DEBUG = properties_dict.get('debug_flag')    import sparkutils    sparkutils.quiet_logs(sc)    sparkutils.addAllPyFiles(sc)    import debugutils
    HiveCtx = HiveContext(sc)    calculate_wc_join_data_final = calculate_wc_join_data(HiveCtx, properties_dict)    calculate_wc_join_data_final.rdd.map(sparkutils.to_csv_line).saveAsTextFile(properties_dict.get('data_item_dir') + "/" + properties_dict.get('workflow_id') + "/" + "wc_join_data")    sc.stop()
